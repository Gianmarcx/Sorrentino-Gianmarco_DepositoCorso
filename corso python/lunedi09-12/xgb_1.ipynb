{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1eab722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import warnings\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da433176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MedInc       20640 non-null  float64\n",
      " 1   HouseAge     20640 non-null  float64\n",
      " 2   AveRooms     20640 non-null  float64\n",
      " 3   AveBedrms    20640 non-null  float64\n",
      " 4   Population   20640 non-null  float64\n",
      " 5   AveOccup     20640 non-null  float64\n",
      " 6   Latitude     20640 non-null  float64\n",
      " 7   Longitude    20640 non-null  float64\n",
      " 8   MedHouseVal  20640 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "#importazione dei dataset\n",
    "raw_data = fetch_california_housing()\n",
    "data = pd.DataFrame(raw_data.data, columns=raw_data.feature_names)\n",
    "data['MedHouseVal'] = raw_data.target\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50ab9f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rimozione outlier\n",
    "numeric_cols = data.select_dtypes(include=[\"float\", \"int\"]).columns.tolist()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = data[col].quantile(0.3)\n",
    "    Q3 = data[col].quantile(0.7)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "\n",
    "    data[col] = np.where(data[col].between(lower, upper), data[col], np.nan)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "#features combinate\n",
    "data['Ave_Room_Bed'] = data['AveRooms'] / data['AveBedrms']\n",
    "data['HouseRooms'] = data['HouseAge'] / data['AveRooms']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6118310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Split interno per validazione durante il tuning\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    # Spazio di ricerca\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse', # Metrica per early stopping e pruning\n",
    "        'tree_method': 'hist',\n",
    "        \n",
    "        # --- Struttura ---\n",
    "        # California ha interazioni complesse, permettiamo alberi più profondi\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
    "        # Importante per evitare overfitting su outlier di prezzo\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 25),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-7, 1.5, log=True),\n",
    "        \n",
    "        # --- Randomness ---\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        \n",
    "        # --- Regolarizzazione ---\n",
    "        # Fondamentale nella regressione per non inseguire i prezzi estremi\n",
    "        'lambda': trial.suggest_float('lambda', 1e-9, 15.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-9, 15.0, log=True),\n",
    "        \n",
    "        # --- Learning ---\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.6),\n",
    "    }\n",
    "\n",
    "    # Pruning basato su RMSE\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\")\n",
    "\n",
    "    model = xgb.train(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dvalid, \"validation\")],\n",
    "        early_stopping_rounds=100,\n",
    "        callbacks=[pruning_callback],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Optuna deve minimizzare RMSE\n",
    "    preds = model.predict(dvalid)\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, preds))\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4e22b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (13561, 10)\n",
      "Target medio: 1.85 (centinaia di k$)\n",
      "\n",
      "--- Training Modello Baseline (Default) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 12:43:44,046] A new study created in memory with name: no-name-2d667dd0-6378-4475-b738-2bbee370f45a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 0.3522\n",
      "\n",
      "--- Inizio Tuning con Optuna ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 12:43:44,803] Trial 0 finished with value: 0.39503958285676205 and parameters: {'max_depth': 16, 'min_child_weight': 15, 'gamma': 0.0004331696672311377, 'subsample': 0.6697426843004406, 'colsample_bytree': 0.6969508372673816, 'lambda': 1.4013164237638986e-05, 'alpha': 6.571085941177902e-06, 'learning_rate': 0.1843866995673897}. Best is trial 0 with value: 0.39503958285676205.\n",
      "[I 2025-12-09 12:43:45,058] Trial 1 finished with value: 0.40832971819430897 and parameters: {'max_depth': 9, 'min_child_weight': 2, 'gamma': 0.015511235821291068, 'subsample': 0.8723785166629177, 'colsample_bytree': 0.8154185789974243, 'lambda': 1.1935998704130544e-09, 'alpha': 0.004731079259346015, 'learning_rate': 0.3941470330447055}. Best is trial 0 with value: 0.39503958285676205.\n",
      "[I 2025-12-09 12:43:46,721] Trial 2 finished with value: 0.35441231638452836 and parameters: {'max_depth': 13, 'min_child_weight': 23, 'gamma': 3.8838420870962745e-06, 'subsample': 0.7570628531701007, 'colsample_bytree': 0.9109736816364161, 'lambda': 0.02307771384295654, 'alpha': 0.9454327224813569, 'learning_rate': 0.06885420667941514}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:43:47,346] Trial 3 finished with value: 0.5114638767842196 and parameters: {'max_depth': 14, 'min_child_weight': 7, 'gamma': 1.1225042973775519e-07, 'subsample': 0.6115291546602176, 'colsample_bytree': 0.9453409907136509, 'lambda': 1.328330822007356e-06, 'alpha': 4.428737155170009e-09, 'learning_rate': 0.580876079269565}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:43:47,821] Trial 4 finished with value: 0.3793727806371193 and parameters: {'max_depth': 4, 'min_child_weight': 24, 'gamma': 0.5833113123796716, 'subsample': 0.8850069971383396, 'colsample_bytree': 0.6821702865351081, 'lambda': 0.004169050075372257, 'alpha': 0.00013151792287752057, 'learning_rate': 0.22782971048237713}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:43:47,879] Trial 5 pruned. Trial was pruned at iteration 6.\n",
      "[I 2025-12-09 12:43:48,308] Trial 6 pruned. Trial was pruned at iteration 128.\n",
      "[I 2025-12-09 12:43:48,326] Trial 7 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:48,984] Trial 8 pruned. Trial was pruned at iteration 128.\n",
      "[I 2025-12-09 12:43:48,995] Trial 9 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:49,013] Trial 10 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:49,024] Trial 11 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:49,035] Trial 12 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:49,543] Trial 13 finished with value: 0.36687110725850625 and parameters: {'max_depth': 5, 'min_child_weight': 18, 'gamma': 4.838482464013621e-07, 'subsample': 0.9583187353913926, 'colsample_bytree': 0.8775525674575203, 'lambda': 0.0003167762427750174, 'alpha': 0.0001594290543081974, 'learning_rate': 0.36207571631342816}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:43:49,838] Trial 14 pruned. Trial was pruned at iteration 128.\n",
      "[I 2025-12-09 12:43:50,100] Trial 15 pruned. Trial was pruned at iteration 52.\n",
      "[I 2025-12-09 12:43:50,834] Trial 16 pruned. Trial was pruned at iteration 117.\n",
      "[I 2025-12-09 12:43:50,925] Trial 17 pruned. Trial was pruned at iteration 15.\n",
      "[I 2025-12-09 12:43:50,941] Trial 18 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:51,151] Trial 19 pruned. Trial was pruned at iteration 103.\n",
      "[I 2025-12-09 12:43:51,303] Trial 20 pruned. Trial was pruned at iteration 21.\n",
      "[I 2025-12-09 12:43:51,315] Trial 21 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:51,328] Trial 22 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:51,663] Trial 23 pruned. Trial was pruned at iteration 128.\n",
      "[I 2025-12-09 12:43:51,683] Trial 24 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:51,695] Trial 25 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:51,940] Trial 26 pruned. Trial was pruned at iteration 103.\n",
      "[I 2025-12-09 12:43:51,957] Trial 27 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:51,969] Trial 28 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:51,987] Trial 29 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:52,001] Trial 30 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:52,022] Trial 31 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:52,143] Trial 32 pruned. Trial was pruned at iteration 18.\n",
      "[I 2025-12-09 12:43:52,164] Trial 33 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:52,310] Trial 34 pruned. Trial was pruned at iteration 18.\n",
      "[I 2025-12-09 12:43:52,441] Trial 35 pruned. Trial was pruned at iteration 29.\n",
      "[I 2025-12-09 12:43:52,649] Trial 36 pruned. Trial was pruned at iteration 19.\n",
      "[I 2025-12-09 12:43:52,679] Trial 37 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:52,699] Trial 38 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:52,721] Trial 39 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:52,733] Trial 40 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:52,823] Trial 41 pruned. Trial was pruned at iteration 26.\n",
      "[I 2025-12-09 12:43:52,947] Trial 42 pruned. Trial was pruned at iteration 34.\n",
      "[I 2025-12-09 12:43:53,082] Trial 43 pruned. Trial was pruned at iteration 27.\n",
      "[I 2025-12-09 12:43:53,255] Trial 44 pruned. Trial was pruned at iteration 128.\n",
      "[I 2025-12-09 12:43:53,468] Trial 45 pruned. Trial was pruned at iteration 59.\n",
      "[I 2025-12-09 12:43:53,524] Trial 46 pruned. Trial was pruned at iteration 31.\n",
      "[I 2025-12-09 12:43:53,537] Trial 47 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:53,641] Trial 48 pruned. Trial was pruned at iteration 27.\n",
      "[I 2025-12-09 12:43:53,656] Trial 49 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:53,683] Trial 50 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:53,767] Trial 51 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 12:43:53,873] Trial 52 pruned. Trial was pruned at iteration 15.\n",
      "[I 2025-12-09 12:43:54,003] Trial 53 pruned. Trial was pruned at iteration 15.\n",
      "[I 2025-12-09 12:43:54,016] Trial 54 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:54,127] Trial 55 pruned. Trial was pruned at iteration 16.\n",
      "[I 2025-12-09 12:43:54,735] Trial 56 pruned. Trial was pruned at iteration 103.\n",
      "[I 2025-12-09 12:43:54,866] Trial 57 pruned. Trial was pruned at iteration 25.\n",
      "[I 2025-12-09 12:43:54,885] Trial 58 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:55,181] Trial 59 pruned. Trial was pruned at iteration 50.\n",
      "[I 2025-12-09 12:43:55,199] Trial 60 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:55,393] Trial 61 pruned. Trial was pruned at iteration 134.\n",
      "[I 2025-12-09 12:43:55,416] Trial 62 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-12-09 12:43:55,649] Trial 63 pruned. Trial was pruned at iteration 128.\n",
      "[I 2025-12-09 12:43:55,662] Trial 64 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:55,676] Trial 65 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:55,755] Trial 66 pruned. Trial was pruned at iteration 32.\n",
      "[I 2025-12-09 12:43:55,774] Trial 67 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:55,788] Trial 68 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:55,989] Trial 69 pruned. Trial was pruned at iteration 128.\n",
      "[I 2025-12-09 12:43:56,007] Trial 70 pruned. Trial was pruned at iteration 3.\n",
      "[I 2025-12-09 12:43:56,021] Trial 71 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-12-09 12:43:56,098] Trial 72 pruned. Trial was pruned at iteration 34.\n",
      "[I 2025-12-09 12:43:56,244] Trial 73 pruned. Trial was pruned at iteration 128.\n",
      "[I 2025-12-09 12:43:56,391] Trial 74 pruned. Trial was pruned at iteration 62.\n",
      "[I 2025-12-09 12:43:56,698] Trial 75 pruned. Trial was pruned at iteration 134.\n",
      "[I 2025-12-09 12:43:57,010] Trial 76 pruned. Trial was pruned at iteration 134.\n",
      "[I 2025-12-09 12:43:57,337] Trial 77 pruned. Trial was pruned at iteration 134.\n",
      "[I 2025-12-09 12:43:57,352] Trial 78 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:57,368] Trial 79 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:57,383] Trial 80 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:57,700] Trial 81 pruned. Trial was pruned at iteration 134.\n",
      "[I 2025-12-09 12:43:57,723] Trial 82 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-12-09 12:43:57,737] Trial 83 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:58,038] Trial 84 pruned. Trial was pruned at iteration 134.\n",
      "[I 2025-12-09 12:43:58,055] Trial 85 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:58,071] Trial 86 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:58,558] Trial 87 pruned. Trial was pruned at iteration 128.\n",
      "[I 2025-12-09 12:43:58,571] Trial 88 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:59,338] Trial 89 finished with value: 0.38642441483069956 and parameters: {'max_depth': 18, 'min_child_weight': 23, 'gamma': 0.000843488002254715, 'subsample': 0.9049491109259614, 'colsample_bytree': 0.8816163738990713, 'lambda': 2.730858897640883e-07, 'alpha': 0.0006404513132409118, 'learning_rate': 0.29982130332604806}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:43:59,359] Trial 90 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:43:59,603] Trial 91 pruned. Trial was pruned at iteration 18.\n",
      "[I 2025-12-09 12:44:00,372] Trial 92 finished with value: 0.38586719608804326 and parameters: {'max_depth': 18, 'min_child_weight': 25, 'gamma': 0.0002711470665519518, 'subsample': 0.8419986543144767, 'colsample_bytree': 0.8890464174504747, 'lambda': 2.3531277994798827e-07, 'alpha': 4.792678691679792e-05, 'learning_rate': 0.307500109082122}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:44:01,193] Trial 93 finished with value: 0.39180094370168494 and parameters: {'max_depth': 18, 'min_child_weight': 25, 'gamma': 0.00037100801190269826, 'subsample': 0.843099530276152, 'colsample_bytree': 0.8899842637562553, 'lambda': 5.01408568973772e-09, 'alpha': 3.323306698683732e-05, 'learning_rate': 0.30407065277416645}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:44:01,215] Trial 94 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:02,049] Trial 95 finished with value: 0.38212087561014113 and parameters: {'max_depth': 18, 'min_child_weight': 25, 'gamma': 0.00012082955544225464, 'subsample': 0.8951669831262947, 'colsample_bytree': 0.8904793634417929, 'lambda': 4.758408755121806e-09, 'alpha': 7.64548864676179e-05, 'learning_rate': 0.30921880499895293}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:44:02,291] Trial 96 pruned. Trial was pruned at iteration 33.\n",
      "[I 2025-12-09 12:44:02,316] Trial 97 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:03,133] Trial 98 finished with value: 0.37914899246528344 and parameters: {'max_depth': 18, 'min_child_weight': 25, 'gamma': 8.018285528198385e-05, 'subsample': 0.8631049520295331, 'colsample_bytree': 0.8896343298835828, 'lambda': 6.177800212521971e-09, 'alpha': 6.305854584571275e-05, 'learning_rate': 0.3077004616639395}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:44:03,293] Trial 99 pruned. Trial was pruned at iteration 22.\n",
      "[I 2025-12-09 12:44:03,316] Trial 100 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:03,352] Trial 101 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-12-09 12:44:03,376] Trial 102 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:03,400] Trial 103 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:04,200] Trial 104 finished with value: 0.38748507766903045 and parameters: {'max_depth': 18, 'min_child_weight': 25, 'gamma': 0.00227476783066362, 'subsample': 0.921222002927819, 'colsample_bytree': 0.8862956254074765, 'lambda': 1.5028894189764331e-09, 'alpha': 2.469269928317796e-05, 'learning_rate': 0.3141968221331546}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:44:05,081] Trial 105 pruned. Trial was pruned at iteration 125.\n",
      "[I 2025-12-09 12:44:05,296] Trial 106 pruned. Trial was pruned at iteration 33.\n",
      "[I 2025-12-09 12:44:05,331] Trial 107 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-12-09 12:44:06,055] Trial 108 finished with value: 0.39059015010813536 and parameters: {'max_depth': 17, 'min_child_weight': 23, 'gamma': 0.00047068062332561894, 'subsample': 0.8928380681409078, 'colsample_bytree': 0.8799201921550619, 'lambda': 4.089305223434742e-09, 'alpha': 1.4487544887319423e-05, 'learning_rate': 0.3628789988985647}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:44:06,076] Trial 109 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:06,191] Trial 110 pruned. Trial was pruned at iteration 14.\n",
      "[I 2025-12-09 12:44:06,302] Trial 111 pruned. Trial was pruned at iteration 14.\n",
      "[I 2025-12-09 12:44:06,338] Trial 112 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-12-09 12:44:06,450] Trial 113 pruned. Trial was pruned at iteration 15.\n",
      "[I 2025-12-09 12:44:06,595] Trial 114 pruned. Trial was pruned at iteration 19.\n",
      "[I 2025-12-09 12:44:06,616] Trial 115 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:06,640] Trial 116 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:06,746] Trial 117 pruned. Trial was pruned at iteration 13.\n",
      "[I 2025-12-09 12:44:06,768] Trial 118 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:07,589] Trial 119 finished with value: 0.39063380336497905 and parameters: {'max_depth': 20, 'min_child_weight': 25, 'gamma': 4.825553374422098e-06, 'subsample': 0.9401346823630957, 'colsample_bytree': 0.8903705685657216, 'lambda': 2.146620346885275e-09, 'alpha': 3.0746514654477e-05, 'learning_rate': 0.3319857804332585}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:44:07,620] Trial 120 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-12-09 12:44:07,644] Trial 121 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:07,755] Trial 122 pruned. Trial was pruned at iteration 13.\n",
      "[I 2025-12-09 12:44:07,870] Trial 123 pruned. Trial was pruned at iteration 13.\n",
      "[I 2025-12-09 12:44:07,893] Trial 124 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:08,562] Trial 125 finished with value: 0.38501683354372657 and parameters: {'max_depth': 17, 'min_child_weight': 24, 'gamma': 0.0021408942252179736, 'subsample': 0.9680356539303222, 'colsample_bytree': 0.8957582053499119, 'lambda': 1.735996073470743e-08, 'alpha': 7.627350634555021e-06, 'learning_rate': 0.34683798957566137}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:44:08,594] Trial 126 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-12-09 12:44:08,633] Trial 127 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-12-09 12:44:08,657] Trial 128 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:08,733] Trial 129 pruned. Trial was pruned at iteration 8.\n",
      "[I 2025-12-09 12:44:08,756] Trial 130 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:08,855] Trial 131 pruned. Trial was pruned at iteration 12.\n",
      "[I 2025-12-09 12:44:08,887] Trial 132 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-12-09 12:44:08,996] Trial 133 pruned. Trial was pruned at iteration 13.\n",
      "[I 2025-12-09 12:44:09,010] Trial 134 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:09,033] Trial 135 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:09,048] Trial 136 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:09,735] Trial 137 finished with value: 0.38905958436715304 and parameters: {'max_depth': 16, 'min_child_weight': 24, 'gamma': 0.0024067413747634672, 'subsample': 0.9030643088926219, 'colsample_bytree': 0.8787789580673724, 'lambda': 8.640599601294926e-09, 'alpha': 0.00011856593543134916, 'learning_rate': 0.34138807019352646}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:44:09,838] Trial 138 pruned. Trial was pruned at iteration 13.\n",
      "[I 2025-12-09 12:44:09,861] Trial 139 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:10,553] Trial 140 finished with value: 0.389288828414957 and parameters: {'max_depth': 16, 'min_child_weight': 22, 'gamma': 0.001651335495215434, 'subsample': 0.8171982150671505, 'colsample_bytree': 0.8863284336238703, 'lambda': 0.12004665212156113, 'alpha': 1.5381064746096658e-05, 'learning_rate': 0.3211947071342697}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:44:10,582] Trial 141 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-12-09 12:44:11,261] Trial 142 finished with value: 0.38728648206562194 and parameters: {'max_depth': 16, 'min_child_weight': 22, 'gamma': 0.0006429237957678839, 'subsample': 0.9058950903368254, 'colsample_bytree': 0.8800752491909256, 'lambda': 0.2129403312120189, 'alpha': 2.7029716230442303e-05, 'learning_rate': 0.35181411443027455}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:44:11,364] Trial 143 pruned. Trial was pruned at iteration 13.\n",
      "[I 2025-12-09 12:44:11,478] Trial 144 pruned. Trial was pruned at iteration 15.\n",
      "[I 2025-12-09 12:44:11,576] Trial 145 pruned. Trial was pruned at iteration 13.\n",
      "[I 2025-12-09 12:44:11,599] Trial 146 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:12,419] Trial 147 pruned. Trial was pruned at iteration 128.\n",
      "[I 2025-12-09 12:44:12,495] Trial 148 pruned. Trial was pruned at iteration 7.\n",
      "[I 2025-12-09 12:44:12,526] Trial 149 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-12-09 12:44:12,597] Trial 150 pruned. Trial was pruned at iteration 8.\n",
      "[I 2025-12-09 12:44:12,629] Trial 151 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-12-09 12:44:12,652] Trial 152 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:12,676] Trial 153 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:12,701] Trial 154 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:12,726] Trial 155 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:12,817] Trial 156 pruned. Trial was pruned at iteration 10.\n",
      "[I 2025-12-09 12:44:12,837] Trial 157 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:12,874] Trial 158 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-12-09 12:44:12,897] Trial 159 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:12,931] Trial 160 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-12-09 12:44:12,946] Trial 161 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:12,985] Trial 162 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-12-09 12:44:13,010] Trial 163 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:13,087] Trial 164 pruned. Trial was pruned at iteration 8.\n",
      "[I 2025-12-09 12:44:13,111] Trial 165 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:13,182] Trial 166 pruned. Trial was pruned at iteration 7.\n",
      "[I 2025-12-09 12:44:13,215] Trial 167 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-12-09 12:44:13,248] Trial 168 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-12-09 12:44:13,341] Trial 169 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-12-09 12:44:13,417] Trial 170 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-12-09 12:44:13,476] Trial 171 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-12-09 12:44:13,536] Trial 172 pruned. Trial was pruned at iteration 3.\n",
      "[I 2025-12-09 12:44:13,590] Trial 173 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 12:44:13,666] Trial 174 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-12-09 12:44:13,692] Trial 175 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:13,731] Trial 176 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:13,788] Trial 177 pruned. Trial was pruned at iteration 4.\n",
      "[I 2025-12-09 12:44:13,812] Trial 178 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:13,919] Trial 179 pruned. Trial was pruned at iteration 7.\n",
      "[I 2025-12-09 12:44:13,941] Trial 180 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:13,958] Trial 181 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:13,973] Trial 182 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:13,994] Trial 183 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:14,068] Trial 184 pruned. Trial was pruned at iteration 7.\n",
      "[I 2025-12-09 12:44:14,088] Trial 185 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:14,107] Trial 186 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:14,620] Trial 187 finished with value: 0.386550808583846 and parameters: {'max_depth': 11, 'min_child_weight': 21, 'gamma': 0.0005247712803261699, 'subsample': 0.9378448007332966, 'colsample_bytree': 0.8730526028511336, 'lambda': 3.4065667440735367e-09, 'alpha': 0.0001105385247890008, 'learning_rate': 0.3409754412501256}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:44:14,665] Trial 188 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-12-09 12:44:14,700] Trial 189 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-12-09 12:44:14,783] Trial 190 pruned. Trial was pruned at iteration 8.\n",
      "[I 2025-12-09 12:44:14,885] Trial 191 pruned. Trial was pruned at iteration 11.\n",
      "[I 2025-12-09 12:44:14,914] Trial 192 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-12-09 12:44:14,950] Trial 193 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-12-09 12:44:15,049] Trial 194 pruned. Trial was pruned at iteration 13.\n",
      "[I 2025-12-09 12:44:15,066] Trial 195 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:15,098] Trial 196 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:15,704] Trial 197 finished with value: 0.38069948793430003 and parameters: {'max_depth': 13, 'min_child_weight': 22, 'gamma': 0.0001990202079632597, 'subsample': 0.8960401373260602, 'colsample_bytree': 0.8769664443767023, 'lambda': 2.0527801780670902e-07, 'alpha': 9.244568793666585e-06, 'learning_rate': 0.3113416844679849}. Best is trial 2 with value: 0.35441231638452836.\n",
      "[I 2025-12-09 12:44:15,727] Trial 198 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 12:44:15,764] Trial 199 pruned. Trial was pruned at iteration 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Miglior RMSE trovato da Optuna (Validation): 0.3544\n",
      "Migliori parametri: {'max_depth': 13, 'min_child_weight': 23, 'gamma': 3.8838420870962745e-06, 'subsample': 0.7570628531701007, 'colsample_bytree': 0.9109736816364161, 'lambda': 0.02307771384295654, 'alpha': 0.9454327224813569, 'learning_rate': 0.06885420667941514}\n",
      "\n",
      "--- Training Modello Finale Ottimizzato ---\n",
      "\n",
      "--- CONFRONTO FINALE ---\n",
      "RMSE Baseline:    0.3522\n",
      "RMSE Ottimizzato: 0.3379\n",
      "--> Miglioramento Errore: 0.0143 (Minore è meglio)\n",
      "------------------------------\n",
      "R2 Score Baseline:    0.8221\n",
      "R2 Score Ottimizzato: 0.8362\n",
      "--> Varianza Spiegata Extra: +1.41%\n"
     ]
    }
   ],
   "source": [
    "#X, y = data.data, data.target\n",
    "X = data.drop(columns=[\"MedHouseVal\"])\n",
    "y= data[\"MedHouseVal\"]\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_full)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dataset Shape: {X.shape}\")\n",
    "print(f\"Target medio: {np.mean(y):.2f} (centinaia di k$)\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Baseline (Senza Tuning)\n",
    "# ==========================================\n",
    "print(\"\\n--- Training Modello Baseline (Default) ---\")\n",
    "dtrain_full = xgb.DMatrix(X_train_full, label=y_train_full)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Parametri standard di default\n",
    "params_base = {\n",
    "    'objective': 'reg:squarederror', # Regressione\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "model_base = xgb.train(params_base, dtrain_full, num_boost_round=100)\n",
    "preds_base = model_base.predict(dtest)\n",
    "\n",
    "# Calcolo RMSE Baseline\n",
    "rmse_base = np.sqrt(mean_squared_error(y_test, preds_base))\n",
    "print(f\"Baseline RMSE: {rmse_base:.4f}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. Ottimizzazione con Optuna\n",
    "# ==========================================\n",
    "print(\"\\n--- Inizio Tuning con Optuna ---\")\n",
    "\n",
    "# Creazione Studio (Minimizzare RMSE)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=200, timeout=600)\n",
    "\n",
    "print(f\"\\nMiglior RMSE trovato da Optuna (Validation): {study.best_value:.4f}\")\n",
    "print(\"Migliori parametri:\", study.best_params)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. Training Modello Finale (Best Params + Shrinkage)\n",
    "# ==========================================\n",
    "print(\"\\n--- Training Modello Finale Ottimizzato ---\")\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params['objective'] = 'reg:squarederror'\n",
    "best_params['eval_metric'] = 'rmse'\n",
    "best_params['tree_method'] = 'hist'\n",
    "\n",
    "# STRATEGIA \"SHRINKAGE\" (Raffiniamo la discesa del gradiente)\n",
    "# Abbassiamo il learning rate trovato per maggiore precisione finale\n",
    "best_params['learning_rate'] = best_params['learning_rate'] * 0.5 \n",
    "# Aumentiamo gli alberi di conseguenza (safety buffer alto, early stopping gestirà il resto)\n",
    "num_round_final = 5000 \n",
    "\n",
    "model_opt = xgb.train(\n",
    "    best_params, \n",
    "    dtrain_full, \n",
    "    num_boost_round=num_round_final,\n",
    "    evals=[(dtest, \"test\")], # Test set usato SOLO per fermare il training al punto giusto\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 5. Analisi Risultati\n",
    "# ==========================================\n",
    "print(\"\\n--- CONFRONTO FINALE ---\")\n",
    "\n",
    "preds_opt = model_opt.predict(dtest)\n",
    "rmse_opt = np.sqrt(mean_squared_error(y_test, preds_opt))\n",
    "\n",
    "# R2 Score (coefficiente di determinazione)\n",
    "r2_base = r2_score(y_test, preds_base)\n",
    "r2_opt = r2_score(y_test, preds_opt)\n",
    "\n",
    "print(f\"RMSE Baseline:    {rmse_base:.4f}\")\n",
    "print(f\"RMSE Ottimizzato: {rmse_opt:.4f}\")\n",
    "print(f\"--> Miglioramento Errore: {rmse_base - rmse_opt:.4f} (Minore è meglio)\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"R2 Score Baseline:    {r2_base:.4f}\")\n",
    "print(f\"R2 Score Ottimizzato: {r2_opt:.4f}\")\n",
    "print(f\"--> Varianza Spiegata Extra: +{(r2_opt - r2_base)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d05c62",
   "metadata": {},
   "source": [
    "**RISULTATO**\n",
    "\n",
    "RMSE Baseline:    0.4718\n",
    "RMSE Ottimizzato: 0.4300\n",
    "\n",
    "param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse', # Metrica per early stopping e pruning\n",
    "        'tree_method': 'hist',\n",
    "        \n",
    "        # --- Struttura ---\n",
    "        # California ha interazioni complesse, permettiamo alberi più profondi\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 15),\n",
    "        # Importante per evitare overfitting su outlier di prezzo\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        \n",
    "        # --- Randomness ---\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        \n",
    "        # --- Regolarizzazione ---\n",
    "        # Fondamentale nella regressione per non inseguire i prezzi estremi\n",
    "        'lambda': trial.suggest_float('lambda', 1e-9, 25.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-9, 25.0, log=True),\n",
    "        \n",
    "        # --- Learning ---\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "    }\n",
    "\n",
    "study.optimize(objective, n_trials=70, timeout=600)\n",
    "\n",
    "=============================================================================== <br>\n",
    "RMSE Baseline:    0.4718\n",
    "RMSE Ottimizzato: 0.4310\n",
    "\n",
    "con scaler\n",
    "\n",
    "param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse', # Metrica per early stopping e pruning\n",
    "        'tree_method': 'hist',\n",
    "        \n",
    "        # --- Struttura ---\n",
    "        # California ha interazioni complesse, permettiamo alberi più profondi\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
    "        # Importante per evitare overfitting su outlier di prezzo\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 25),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-9, 1.5, log=True),\n",
    "        \n",
    "        # --- Randomness ---\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        \n",
    "        # --- Regolarizzazione ---\n",
    "        # Fondamentale nella regressione per non inseguire i prezzi estremi\n",
    "        'lambda': trial.suggest_float('lambda', 1e-6, 20.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-6, 20.0, log=True),\n",
    "        \n",
    "        # --- Learning ---\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.8),\n",
    "    }\n",
    "\n",
    "study.optimize(objective, n_trials=200, timeout=600)\n",
    "\n",
    "=============================================================================== <br>\n",
    "\n",
    "RMSE Baseline:    0.3522\n",
    "RMSE Ottimizzato: 0.3379\n",
    "\n",
    "con scaler\n",
    "\n",
    "study.optimize(objective, n_trials=200, timeout=600)\n",
    "\n",
    "param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse', # Metrica per early stopping e pruning\n",
    "        'tree_method': 'hist',\n",
    "        \n",
    "        # --- Struttura ---\n",
    "        # California ha interazioni complesse, permettiamo alberi più profondi\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
    "        # Importante per evitare overfitting su outlier di prezzo\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 25),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-7, 1.5, log=True),\n",
    "        \n",
    "        # --- Randomness ---\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        \n",
    "        # --- Regolarizzazione ---\n",
    "        # Fondamentale nella regressione per non inseguire i prezzi estremi\n",
    "        'lambda': trial.suggest_float('lambda', 1e-9, 15.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-9, 15.0, log=True),\n",
    "        \n",
    "        # --- Learning ---\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.6),\n",
    "    }\n",
    "\n",
    "#rimozione outlier\n",
    "numeric_cols = data.select_dtypes(include=[\"float\", \"int\"]).columns.tolist()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = data[col].quantile(0.3)\n",
    "    Q3 = data[col].quantile(0.7)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "\n",
    "    data[col] = np.where(data[col].between(lower, upper), data[col], np.nan)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "#features combinate\n",
    "data['Ave_Room_Bed'] = data['AveRooms'] / data['AveBedrms']\n",
    "data['HouseRooms'] = data['HouseAge'] / data['AveRooms']\n",
    "\n",
    "Miglior RMSE trovato da Optuna (Validation): 0.3544\n",
    "Migliori parametri: {'max_depth': 13, 'min_child_weight': 23, 'gamma': 3.8838420870962745e-06, 'subsample': 0.7570628531701007, 'colsample_bytree': 0.9109736816364161, 'lambda': 0.02307771384295654, 'alpha': 0.9454327224813569, 'learning_rate': 0.06885420667941514}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
