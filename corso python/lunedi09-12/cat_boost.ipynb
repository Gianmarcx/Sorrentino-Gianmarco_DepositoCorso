{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07d1e00",
   "metadata": {},
   "source": [
    "\n",
    "## **1. Introduzione Tecnica: Il Motore sotto il Cofano**\n",
    "\n",
    "Molti praticanti usano XGBoost come una \"black box\", ma l'efficacia del tuning dipende dalla comprensione di due pilastri: l'approccio matematico di **secondo ordine** e l'**ottimizzazione di sistema**.\n",
    "\n",
    "### **1.1 Oltre il classico Gradient Boosting (GBM)**\n",
    "\n",
    "Il Gradient Boosting standard è un metodo \"additivo\". Invece di ottimizzare i parametri di un singolo modello complesso (come in una rete neurale), costruiamo un insieme di modelli \"deboli\" (alberi decisionali) in sequenza.\n",
    "\n",
    "Matematicamente, la predizione al passo $t$ per l'istanza $i$ è data da:\n",
    "\n",
    "$$\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + \\eta f_t(x_i)$$\n",
    "\n",
    "Dove:\n",
    "* $\\hat{y}_i^{(t-1)}$ è la predizione accumulata fino al passo precedente.\n",
    "* $f_t(x_i)$ è il nuovo albero che stiamo aggiungendo.\n",
    "* $\\eta$ è il learning rate.\n",
    "\n",
    "\n",
    "\n",
    "**La differenza cruciale:**\n",
    "In un GBM standard, il nuovo albero $f_t$ viene addestrato per predire i **residui** (o il gradiente negativo della loss) del modello precedente. Usa il metodo della *Discesa del Gradiente* (Primo Ordine).\n",
    "\n",
    "**XGBoost**, invece, utilizza il **Metodo di Newton** (Secondo Ordine). Quando ottimizza la funzione obiettivo per trovare il miglior albero successivo, non guarda solo la pendenza (gradiente), ma anche la **curvatura** della funzione di loss.\n",
    "\n",
    "### **1.2 L'Espansione di Taylor: Il cuore di XGBoost**\n",
    "\n",
    "Per capire i parametri che vedremo dopo (come `min_child_weight`), dobbiamo guardare l'approssimazione della Loss Function che XGBoost utilizza.\n",
    "\n",
    "XGBoost approssima la funzione obiettivo usando l'**Espansione di Taylor di secondo ordine**:\n",
    "\n",
    "$$\\mathcal{L}^{(t)} \\approx \\sum_{i=1}^n \\left[ l(y_i, \\hat{y}^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2}h_i f_t^2(x_i) \\right] + \\Omega(f_t)$$\n",
    "\n",
    "Qui risiedono i due concetti più importanti per un utente avanzato:\n",
    "\n",
    "1.  **$g_i$ (Gradiente):** La derivata prima (la direzione verso cui muoversi).\n",
    "2.  **$h_i$ (Hessiana):** La derivata seconda (la curvatura). Rappresenta quanto siamo \"sicuri\" o quanto rapidamente sta cambiando il gradiente.\n",
    "\n",
    "> **Insight da Esperto:** Quando tunerai `min_child_weight`, starai letteralmente impostando una soglia sulla somma delle Hessiane ($h_i$) in una foglia. In una regressione MSE, l'Hessiana è costante ($2$), quindi `min_child_weight` è semplicemente il numero di campioni. Ma in una classificazione logistica, l'Hessiana diventa piccola quando il modello è molto sicuro (probabilità vicine a 0 o 1). Ecco perché XGBoost è così preciso: **pesa gli errori in base alla \"sicurezza\" del modello corrente.**\n",
    "\n",
    "### **1.3 Regolarizzazione Integrata ($\\Omega$)**\n",
    "\n",
    "A differenza di altre librerie che applicano la regolarizzazione come ripensamento (post-pruning), XGBoost include il termine di regolarizzazione $\\Omega(f_t)$ direttamente nella funzione obiettivo durante la costruzione dell'albero:\n",
    "\n",
    "$$\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda ||w||^2$$\n",
    "\n",
    "* **$\\gamma T$**: Penalizza il numero di foglie ($T$). Più foglie = costo maggiore. Questo è controllato dal parametro `gamma`.\n",
    "* **$\\lambda ||w||^2$**: Penalizza la grandezza dei pesi delle foglie ($w$, i valori predetti nelle foglie). Questo è controllato dal parametro `lambda`.\n",
    "\n",
    "L'albero non cresce se il guadagno di loss non supera la penalità della complessità.\n",
    "\n",
    "### **1.4 Ottimizzazione di Sistema (Engineering)**\n",
    "\n",
    "XGBoost non è famoso solo per la matematica, ma per come gestisce l'hardware:\n",
    "\n",
    "* **Block Structure & Parallelization:** Un errore comune è pensare che XGBoost costruisca alberi in parallelo. Non può (è sequenziale). Invece, parallelizza la **costruzione dei nodi**. I dati sono pre-ordinati e salvati in blocchi di memoria (CSC format), permettendo a più thread di calcolare i migliori split simultaneamente.\n",
    "* **Weighted Quantile Sketch:** Per dataset enormi, è impossibile testare ogni possibile valore di split. XGBoost usa un algoritmo di sketch approssimato per trovare i migliori punti di taglio candidati, pesati dall'Hessiana ($h_i$).\n",
    "* **Sparsity Awareness:** XGBoost impara una \"direzione di default\" per ogni nodo. Se un dato è mancante (NaN), viene instradato automaticamente nella direzione che minimizza l'errore di training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb047b",
   "metadata": {},
   "source": [
    "## **2. Anatomia Pratica degli Iperparametri (Il \"Booster\")**\n",
    "\n",
    "In un contesto avanzato, non si settano i parametri a caso. Bisogna vedere gli iperparametri come manopole che controllano tre aspetti: **Capacità del modello**, **Robustezza al rumore** e **Velocità di convergenza**.\n",
    "\n",
    "Ecco come un esperto configura il \"Booster\" (`gbtree`), andando oltre i valori di default.\n",
    "\n",
    "### **2.1 Controllo della Struttura (Il freno e l'acceleratore)**\n",
    "\n",
    "Questi parametri definiscono la \"forma\" fisica dei tuoi alberi.\n",
    "\n",
    "* **`max_depth` (Profondità dell'albero)**\n",
    "    * **Cosa fa davvero:** Controlla la complessità delle interazioni tra le feature che il modello può apprendere. Un albero di profondità $N$ può catturare interazioni fino a $N$ variabili.\n",
    "    * **Utilizzo Avanzato:**\n",
    "        * **Non esagerare:** In XGBoost, a differenza di Random Forest, gli alberi non devono essere profondi. Spesso un range **3-6** è ottimale.\n",
    "        * **Segnale vs Rumore:** Se aumenti la profondità sopra a 8-10, stai quasi certamente memorizzando il rumore, a meno che tu non abbia milioni di righe e interazioni feature estremamente complesse.\n",
    "        * **Interazione:** È fortemente correlato a `min_child_weight`. Se alzi la profondità, *devi* alzare `min_child_weight` per evitare foglie con pochi dati.\n",
    "\n",
    "* **`min_child_weight` (Il \"Filtro anti-rumore\")**\n",
    "    * **Cosa fa davvero:** È la somma minima dei pesi (hessiana) necessaria per mantenere un nodo figlio. In pratica (per regressione/classificazione standard), puoi pensarlo come il numero minimo di istanze necessarie in una foglia per validare uno split.\n",
    "    * **Utilizzo Avanzato:**\n",
    "        * **Dataset sbilanciati:** Questo è il parametro più critico. Se hai classi rare, un valore alto impedirà al modello di isolarle.\n",
    "        * **Dataset rumorosi:** Se il tuo dataset ha molto rumore (es. dati finanziari o sensori IoT), alza questo valore (es. 10, 20 o anche 100). Costringe l'albero a fare split solo su pattern molto \"solidi\" e frequenti.\n",
    "        * **Rule of Thumb:** Inizia con 1. Se vedi overfitting massiccio (Train score >> Test score), prova subito a saltare a 5 o 10.\n",
    "\n",
    "* **`gamma` (o `min_split_loss`) - Il Pruning Aggressivo**\n",
    "    * **Cosa fa davvero:** È una soglia \"hard\". Se lo split non riduce la loss function di almeno `gamma`, lo split non avviene. È una regolarizzazione che agisce *durante* la costruzione, non dopo.\n",
    "    * **Utilizzo Avanzato:**\n",
    "        * **Default vs Realtà:** Il default è 0 (crescita greedy).\n",
    "        * **Quando usarlo:** È utilissimo quando il modello continua a creare rami inutili che migliorano di pochissimo la performance. Impostare un gamma basso (es. 0.1 - 0.5) rende il modello molto più conservativo (ottimo per evitare overfitting in produzione).\n",
    "        * **Tuning:** È difficile da tunare con GridSearch perché dipende dalla scala della tua loss. Meglio lasciarlo a 0 all'inizio e alzarlo solo se la regolarizzazione standard (`lambda`/`alpha`) non basta.\n",
    "\n",
    "### **2.2 Campionamento Stocastico (La diversità)**\n",
    "\n",
    "Questi parametri introducono casualità. Senza di questi, XGBoost è deterministico. La casualità riduce la correlazione tra gli alberi, migliorando l'ensemble (meno varianza).\n",
    "\n",
    "* **`subsample` (Righe)**\n",
    "    * **Pratica:** Percentuale di righe campionate per costruire ogni albero.\n",
    "    * **Sweet Spot:** Generalmente tra **0.6 e 0.9**.\n",
    "    * **Warning:** Non scendere mai sotto 0.5 a meno che il dataset non sia enorme. Impostarlo a 1.0 (default) spesso porta a overfitting perché ogni albero vede esattamente gli stessi dati.\n",
    "\n",
    "* **`colsample_bytree` (Colonne)**\n",
    "    * **Pratica:** Percentuale di colonne (feature) scelte a caso per costruire ogni albero. Simile al `max_features` di Random Forest.\n",
    "    * **Utilizzo Avanzato:**\n",
    "        * Questo è spesso **più efficace della regolarizzazione L1/L2**.\n",
    "        * Se hai molte feature collineari (ridondanti), abbassa questo valore (es. 0.6). Costringe gli alberi a usare feature diverse, rendendo il modello finale più robusto se una feature dovesse \"rompersi\" in produzione.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **2.3 La Strategia di Tuning Professionale (\"The Recipe\")**\n",
    "\n",
    "Non lanciare una GridSearch cieca su tutti i parametri. Sprechi CPU. Usa questo approccio a imbuto:\n",
    "\n",
    "1.  **Fase 1: Baseline Veloce**\n",
    "    * Fissa un `learning_rate` alto (es. 0.1 o 0.2) per addestrare velocemente.\n",
    "    * Trova il numero ottimale di alberi (`n_estimators`) usando `early_stopping`.\n",
    "\n",
    "2.  **Fase 2: Struttura dell'Albero (Macro-tuning)**\n",
    "    * Tuna `max_depth` e `min_child_weight` insieme. Sono i parametri che impattano di più sul risultato.\n",
    "    * *Esempio:* GridSearch su Depth [3, 5, 7, 9] e Child Weight [1, 3, 5].\n",
    "\n",
    "3.  **Fase 3: Regolazione fine (Micro-tuning)**\n",
    "    * Tuna `gamma` per potare i rami inutili.\n",
    "    * Tuna `subsample` e `colsample_bytree` per aggiungere robustezza.\n",
    "\n",
    "4.  **Fase 4: Il \"Grand Finale\" (Lower Rate, More Trees)**\n",
    "    * Una volta trovati i parametri strutturali, abbassa il `learning_rate` (es. a 0.01 o 0.005).\n",
    "    * Aumenta proporzionalmente `n_estimators`.\n",
    "    * *Nota da esperto:* Questo passaggio da solo regala spesso un boost di 1-2% di performance, perché permette all'algoritmo di convergere verso il minimo globale con passi più fini, riducendo l'errore residuo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d068a0",
   "metadata": {},
   "source": [
    "## **3. Parametri di Apprendimento e Regolarizzazione**\n",
    "\n",
    "Qui gestiamo due aspetti critici: **come** il modello impara dai propri errori (Learning Rate) e **come** evita di dare troppa importanza a feature rumorose (Regolarizzazione).\n",
    "\n",
    "### **3.1 Il Motore del Boosting: `eta` e `n_estimators`**\n",
    "\n",
    "Questi due parametri vivono in simbiosi. Non puoi modificarne uno senza considerare l'altro.\n",
    "\n",
    "* **`eta` (o `learning_rate`)**\n",
    "    * **Concetto Pratico:** È la \"dimensione del passo\". Dopo ogni albero, XGBoost non aggiunge l'intero valore della predizione, ma lo moltiplica per `eta`. Questo riduce l'impatto di ogni singolo albero, lasciando spazio agli alberi successivi per correggere gli errori.\n",
    "    * **Perché è importante:** Un learning rate basso rende il modello più robusto all'overfitting, poiché la costruzione del modello finale è più graduale e meno dipendente dai primi alberi (che potrebbero aver memorizzato rumore).\n",
    "\n",
    "* **`n_estimators` (o `num_boost_round`)**\n",
    "    * **Concetto Pratico:** Il numero totale di alberi sequenziali da costruire.\n",
    "\n",
    "\n",
    "\n",
    "#### **La Strategia Professionale: \"Shrinkage\"**\n",
    "La regola d'oro nell'industria non è cercare il \"learning rate magico\", ma seguire questa procedura:\n",
    "\n",
    "1.  **Fase di Tuning:** Usa un `eta` alto (es. **0.1** o **0.2**) e un numero di stimatori basso/medio. Questo ti permette di fare decine di test di `max_depth` e `subsample` in pochi minuti invece che ore.\n",
    "2.  **Fase di Produzione:** Una volta trovata la struttura ideale degli alberi, applica la tecnica dello **Shrinkage**:\n",
    "    * Riduci `eta` di un fattore $X$ (es. da 0.1 a **0.01**).\n",
    "    * Aumenta `n_estimators` dello stesso fattore $X$.\n",
    "    * *Risultato:* Il modello impiegherà più tempo ad addestrarsi, ma l'errore di generalizzazione scenderà quasi sempre, migliorando la precisione finale.\n",
    "\n",
    "> **Nota:** Usa sempre **Early Stopping** quando aumenti gli stimatori. Se imposti 10.000 alberi ma il modello smette di migliorare al 3.500esimo, l'early stopping fermerà il training, risparmiando ore di calcolo e prevenendo l'overfitting tardivo.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2 Regolarizzazione Esplicita (L1 & L2)**\n",
    "\n",
    "Mentre `max_depth` limita la struttura, `lambda` e `alpha` limitano i **pesi** numerici assegnati alle foglie. Questo è fondamentale quando si hanno feature con alta varianza o dataset con molto rumore.\n",
    "\n",
    "* **`lambda` (Regolarizzazione L2 - Ridge)**\n",
    "    * **Default:** 1 (Attivo di default, a differenza di sklearn che spesso non regolarizza).\n",
    "    * **Effetto Pratico:** \"Schiaccia\" i valori delle foglie verso zero in modo fluido. Penalizza i valori estremi.\n",
    "    * **Quando usarlo:** È il tuo \"scudo\" standard. Se vedi che il modello dà pesi enormi a certe predizioni su pochi casi, aumenta `lambda`. Aiuta a gestire la multicollinearità (feature correlate).\n",
    "\n",
    "* **`alpha` (Regolarizzazione L1 - Lasso)**\n",
    "    * **Default:** 0.\n",
    "    * **Effetto Pratico:** Forza i pesi delle feature inutili a diventare **esattamente zero**.\n",
    "    * **Quando usarlo (Feature Selection Implicita):**\n",
    "        * Se hai un dataset con **migliaia di feature** (es. One-Hot Encoding di variabili categoriche ad alta cardinalità o dati genomici) e sospetti che solo poche siano importanti.\n",
    "        * Impostare `alpha` alto rende il modello \"sparso\" (più leggero e veloce in inferenza perché usa meno feature).\n",
    "\n",
    "#### **Scenario d'uso: L1 vs L2**\n",
    "* *Problema classico (es. Churn Prediction):* Usa **L2 (`lambda`)**. Vogliamo considerare tutte le variabili un po'.\n",
    "* *Problema ad alta dimensione (es. Analisi del testo con TF-IDF):* Usa **L1 (`alpha`)**. Vogliamo eliminare le migliaia di parole che non servono a nulla.\n",
    "\n",
    "---\n",
    "\n",
    "**Sintesi del Punto 3:**\n",
    "* **Learning Rate:** Tienilo basso in produzione (0.01 - 0.05).\n",
    "* **Estimators:** Alzali quando abbassi il learning rate (e usa Early Stopping).\n",
    "* **Alpha/Lambda:** Usa Alpha se vuoi selezionare feature, Lambda per stabilità generale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a742c",
   "metadata": {},
   "source": [
    "\n",
    "## **4. Gestione di Scenari Complessi**\n",
    "\n",
    "### **4.1 Dataset Sbilanciati (Imbalanced Learning)**\n",
    "Quando la classe positiva (quella che ci interessa, es. \"Frode\" o \"Guasto\") è molto rara, il modello tende a ignorarla per massimizzare l'accuratezza globale (dicendo sempre \"Non Frode\").\n",
    "\n",
    "Ecco come forzare XGBoost a prestare attenzione ai casi rari:\n",
    "\n",
    "* **`scale_pos_weight` (Il Bilanciatore)**\n",
    "    * **La Logica:** Modifica il calcolo del gradiente. Se questo parametro è > 1, gli errori commessi sulla classe positiva pesano di più durante l'aggiornamento dei pesi.\n",
    "    * **Formula Magica:**\n",
    "        $$scale\\_pos\\_weight = \\frac{\\text{Numero di Negativi}}{\\text{Numero di Positivi}}$$\n",
    "    * **Utilizzo Professionale:**\n",
    "        * Calcola questo rapporto e inseriscilo nel modello. Spesso è più efficace e veloce delle tecniche di campionamento esterne (come SMOTE o Oversampling) perché non altera la distribuzione dei dati, ma solo la penalità matematica.\n",
    "        * **Attenzione alle Probabilità:** Quando usi `scale_pos_weight`, le probabilità predette (`predict_proba`) non saranno più calibrate (saranno spostate verso l'alto). Se hai bisogno della probabilità reale (es. per calcolare il rischio finanziario esatto), dovrai ricalibrarle a valle.\n",
    "\n",
    "* **`max_delta_step` (La cintura di sicurezza per la convergenza)**\n",
    "    * **Il Problema:** In scenari estremamente sbilanciati, l'aggiornamento dei pesi di un singolo albero può essere enorme (\"esplosivo\"), portando a instabilità numerica.\n",
    "    * **Soluzione:** `max_delta_step` pone un tetto massimo al cambiamento di peso (delta) di una singola foglia.\n",
    "    * **Configurazione:** Il default è 0 (nessun tetto). Se hai problemi di convergenza con classi molto rare, impostalo tra **1 e 10**. Questo rende l'aggiornamento più conservativo e stabile.\n",
    "\n",
    "\n",
    "\n",
    "### **4.2 Gestione dei Valori Mancanti (Sparsity Awareness)**\n",
    "\n",
    "Molti ingegneri perdono tempo a imputare i valori mancanti (Mean, Median, KNN Imputation) prima di passare i dati a XGBoost. Spesso, **questo è un errore**.\n",
    "\n",
    "* **Sparsity Aware Split Finding**\n",
    "    * XGBoost gestisce i `NaN` (Not a Number) nativamente. Non li ignora, li *usa*.\n",
    "    * **Come funziona:** Per ogni nodo (decisione) nell'albero, l'algoritmo testa due scenari:\n",
    "        1.  Manda tutti i dati con valore mancante a **Sinistra**. Calcola il guadagno di info.\n",
    "        2.  Manda tutti i dati con valore mancante a **Destra**. Calcola il guadagno di info.\n",
    "    * La direzione che minimizza la loss viene \"imparata\" e salvata come **Default Direction**.\n",
    "    * **Perché è geniale:** Spesso un dato mancante non è casuale (Missing Not At Random).\n",
    "        * *Esempio:* In un dataset bancario, se il campo \"Debito Pregresso\" è vuoto (`NaN`), potrebbe significare che il cliente non ha mai avuto debiti (ottimo pagatore). Se lo riempi con la media, distruggi questa informazione. XGBoost invece impara che `NaN` -> \"Ramo dei buoni pagatori\".\n",
    "\n",
    "\n",
    "\n",
    "#### **Best Practice sui Dati Mancanti:**\n",
    "1.  **Non imputare nulla** inizialmente. Passa i `NaN` (o `np.nan` in Python) direttamente a XGBoost.\n",
    "2.  Lascia che l'algoritmo scopra se l'assenza di informazione è essa stessa un'informazione (informative missingness).\n",
    "3.  Imputa manualmente solo se sai per certo che il dato manca per un errore tecnico casuale e la media/mediana è una stima affidabile.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9134161c",
   "metadata": {},
   "source": [
    "## **5. Tecniche Avanzate: Ingegnerizzare la Conoscenza**\n",
    "\n",
    "### **5.1 Monotonic Constraints (Vincoli Monotoni)**\n",
    "Spesso sappiamo a priori come una feature dovrebbe influenzare il target.\n",
    "* *Esempio Immobiliare:* A parità di altre condizioni, se i metri quadri aumentano, il prezzo *deve* salire (o rimanere uguale). Non può scendere.\n",
    "* *Il Problema:* Se il training set ha dei dati rumorosi (es. una casa grande svenduta per urgenza), un albero normale potrebbe imparare un \"dip\" (calo) di prezzo per metrature alte. Questo è overfitting su rumore locale.\n",
    "\n",
    "**La Soluzione:**\n",
    "Puoi forzare XGBoost a rispettare una relazione sempre crescente o decrescente per specifiche feature.\n",
    "\n",
    "* **Parametro:** `monotone_constraints`\n",
    "* **Valori:**\n",
    "    * `1`: Relazione crescente (All'aumentare di X, Y aumenta o resta uguale).\n",
    "    * `-1`: Relazione decrescente (All'aumentare di X, Y diminuisce o resta uguale).\n",
    "    * `0`: Nessun vincolo.\n",
    "* **Vantaggi:**\n",
    "    1.  **Migliore Generalizzazione:** Il modello ignora il rumore che contraddice la logica nota.\n",
    "    2.  **Explainability & Trust:** Quando spieghi il modello agli stakeholder, non vedranno comportamenti illogici (es. \"Perché se guadagno di più la banca mi dà meno credito?\").\n",
    "\n",
    "\n",
    "\n",
    "### **5.2 Interaction Constraints (Vincoli di Interazione)**\n",
    "Di default, XGBoost può combinare qualsiasi feature con qualsiasi altra. Ma in certi settori (es. Assicurativo, Credit Risk), alcune interazioni potrebbero essere vietate per legge (discriminazione) o prive di senso logico.\n",
    "\n",
    "* **Parametro:** `interaction_constraints`\n",
    "* **Come funziona:** Passi una lista di liste. Le feature presenti nella stessa sottolista possono interagire tra loro, ma non con feature di altre liste.\n",
    "    * *Esempio:* `[[Feature_A, Feature_B], [Feature_C, Feature_D, Feature_E]]`.\n",
    "    * Qui l'albero può fare split su A e poi su B nello stesso ramo. Ma se ha fatto split su C, non può scendere e fare uno split su A.\n",
    "* **Uso Avanzato:** Riduce drasticamente lo spazio delle ipotesi, prevenendo il modello dal trovare correlazioni spurie complesse che non esistono nella realtà.\n",
    "\n",
    "### **5.3 Custom Objective Functions (Loss Personalizzate)**\n",
    "Questo è il livello \"Gran Maestro\". A volte `RMSE` (Regressione) o `LogLoss` (Classificazione) non riflettono il vero obiettivo di business.\n",
    "\n",
    "* **Scenario (Asymmetric Loss):** Immagina di predire la domanda di magazzino.\n",
    "    * Sottostimare la domanda (Stock-out) costa 1000€ in vendite perse.\n",
    "    * Sovrastimare la domanda (Over-stock) costa 50€ di stoccaggio.\n",
    "    * L'errore quadratico medio (MSE) tratterebbe +10 e -10 allo stesso modo. Per il business, -10 è un disastro.\n",
    "\n",
    "* **Implementazione:**\n",
    "    XGBoost permette di definire una funzione Python personalizzata che calcola e restituisce due vettori per ogni istanza di training:\n",
    "    1.  **Gradiente (Gradient):** La derivata prima (Direzione dell'errore).\n",
    "    2.  **Hessiana (Hessian):** La derivata seconda (Curvatura/Peso dell'errore).\n",
    "\n",
    "Definendo una funzione che penalizza fortemente il gradiente quando l'errore è negativo (stock-out) e poco quando è positivo, guidi l'apprendimento verso una strategia di \"prudente sovrastima\".\n",
    "\n",
    "### **5.4 Supporto Nativo per Feature Categoriche**\n",
    "Fino a poco tempo fa, dovevi fare *One-Hot Encoding* (OHE) prima di usare XGBoost. Questo creava matrici sparse enormi e alberi sbilanciati (perché per isolare la categoria \"Z\" servivano tanti split se usavi OHE).\n",
    "\n",
    "* **La Novità:** XGBoost ora supporta `enable_categorical=True` (con tree method `hist` o `gpu_hist`).\n",
    "* **Come funziona (Optimal Partitioning):** Invece di trattare le categorie come numeri, l'algoritmo cerca la partizione ottimale delle categorie in due gruppi a ogni nodo.\n",
    "    * *Split:* \"È la categoria {A, C, F}?\" vs \"È la categoria {B, D, E}?\".\n",
    "* **Vantaggio:**\n",
    "    * Addestramento molto più veloce su dataset con alta cardinalità.\n",
    "    * Spesso performance superiori rispetto a OHE o Label Encoding, specialmente con alberi poco profondi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db42bb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **6. Strategie di Tuning e Best Practices (Il Metodo Optuna)**\n",
    "\n",
    "### **6.1 Perché Optuna cambia le regole del gioco**\n",
    "\n",
    "`GridSearch` prova tutte le combinazioni ciecamente. `Optuna` impara dal passato.\n",
    "Usa un algoritmo chiamato **TPE (Tree-structured Parzen Estimator)**.\n",
    "\n",
    "In parole semplici:\n",
    "\n",
    "1.  Optuna lancia un training con parametri a caso.\n",
    "2.  Osserva il risultato.\n",
    "3.  Costruisce un modello probabilistico interno che dice: \"Quando `max_depth` è basso e `eta` è alto, il modello fa schifo. Non proverò più lì. Invece, sembra che `subsample` alto funzioni bene, esplorerò di più quella zona.\"\n",
    "4.  **Pruning (Potatura):** Se un trial (tentativo) sta andando male dopo 10 iterazioni, Optuna lo uccide subito. Non spreca risorse per arrivare alla fine di un training fallimentare.\n",
    "\n",
    "### **6.2 Definire lo Spazio di Ricerca (Search Space)**\n",
    "\n",
    "Prima del codice, ecco come un esperto definisce i range. Nota l'uso della scala logaritmica.\n",
    "\n",
    "  * `learning_rate`: **Logaritmicamente** tra 1e-3 e 0.3. È fondamentale perché l'impatto varia per ordini di grandezza.\n",
    "  * `max_depth`: Intero tra 3 e 10 (o 12).\n",
    "  * `min_child_weight`: Intero tra 1 e 10 (per il rumore).\n",
    "  * `subsample` / `colsample_bytree`: Float tra 0.5 e 1.0.\n",
    "  * `reg_lambda` / `reg_alpha`: **Logaritmicamente** tra 1e-8 e 10.0.\n",
    "\n",
    "### **6.3 Il Codice: Template Professionale per XGBoost + Optuna**\n",
    "\n",
    "Questo script non è solo un esempio, è un template pronto per la produzione. Include il **Pruning**, che è la parte che accelera il tuning del 50-70%.\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Definizione della Funzione Obiettivo\n",
    "def objective(trial, X, y):\n",
    "    \n",
    "    # A. Split veloce per validazione interna al trial\n",
    "    # Nota: In produzione, potresti usare StratifiedKFold qui dentro per maggiore robustezza\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)\n",
    "    \n",
    "    # B. Definizione dello Spazio degli Iperparametri (Dynamic Search Space)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic', # o 'reg:squarederror' per regressione\n",
    "        'tree_method': 'hist',          # Usa 'gpu_hist' se hai una GPU! Velocizza di 10x.\n",
    "        \n",
    "        # Struttura dell'albero\n",
    "        # Suggeriamo un intero per la profondità\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        # Suggeriamo un intero per il peso minimo (controllo rumore/outlier)\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        # Gamma: soglia minima di riduzione loss per split\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        \n",
    "        # Campionamento (Stochastic Gradient Boosting)\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        \n",
    "        # Regolarizzazione (Scala Logaritmica è cruciale qui)\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "        \n",
    "        # Learning Rate e Estimators\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': 1000 # Impostiamo un tetto alto, l'early stopping lo fermerà prima\n",
    "    }\n",
    "\n",
    "    # C. Inizializzazione del Pruner (Interruzione anticipata dei trial scarsi)\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation_0-logloss\")\n",
    "    \n",
    "    # D. Addestramento del Modello\n",
    "    model = xgb.XGBClassifier(**param)\n",
    "    \n",
    "    model.fit(\n",
    "        train_x, \n",
    "        train_y, \n",
    "        eval_set=[(valid_x, valid_y)], \n",
    "        eval_metric=\"logloss\",\n",
    "        early_stopping_rounds=50, # Ferma se non migliora per 50 round\n",
    "        callbacks=[pruning_callback], # Collega Optuna a XGBoost\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # E. Predizione e calcolo metrica da ottimizzare\n",
    "    preds = model.predict_proba(valid_x)[:, 1] # Probabilità classe 1\n",
    "    loss = log_loss(valid_y, preds)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Esecuzione dello Studio\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Supponiamo di avere X e y caricati\n",
    "# X, y = load_data(...) \n",
    "\n",
    "# 1. Creiamo lo studio (Direction: Minimize LogLoss)\n",
    "study = optuna.create_study(\n",
    "    direction='minimize', \n",
    "    sampler=optuna.samplers.TPESampler(), # Sampler Bayesiano standard\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5) # Uccide i trial peggiori della media\n",
    ")\n",
    "\n",
    "# 2. Avviamo l'ottimizzazione\n",
    "# n_trials: Quante combinazioni provare\n",
    "# timeout: Tempo massimo in secondi (es. 1 ora = 3600)\n",
    "print(\"Inizio Tuning con Optuna...\")\n",
    "study.optimize(lambda trial: objective(trial, X, y), n_trials=100, timeout=3600)\n",
    "\n",
    "# 3. Risultati\n",
    "print(\"Migliori parametri:\", study.best_params)\n",
    "print(\"Miglior Loss:\", study.best_value)\n",
    "\n",
    "# 4. Importanza degli Iperparametri\n",
    "# Optuna ti dice quali parametri hanno influito di più sul risultato!\n",
    "optuna.visualization.plot_param_importances(study).show()\n",
    "```\n",
    "\n",
    "### **6.4 Analisi del Codice: I Dettagli che contano**\n",
    "\n",
    "1.  **`suggest_float(..., log=True)`**: Questa è la chiave. Per parametri come `alpha` o `learning_rate`, la differenza tra 0.001 e 0.01 è enorme (10x), mentre tra 0.8 e 0.81 è nulla. La scala logaritmica permette a Optuna di esplorare gli ordini di grandezza in modo efficiente.\n",
    "2.  **`XGBoostPruningCallback`**: Senza questo, Optuna aspetterebbe la fine dei 1000 alberi per ogni trial. Con questo, se al 50° albero la loss è peggiore della media degli altri trial, Optuna lancia un'eccezione, ferma il training e passa al prossimo set di parametri.\n",
    "3.  **`objective` Function**: Nota come tutto (definizione parametri, training, valutazione) avviene dentro questa funzione. Optuna la chiama ripetutamente.\n",
    "4.  **`MedianPruner`**: Una strategia di pruning semplice ma efficace. Se il trial corrente sta andando peggio della mediana dei trial precedenti allo stesso step, viene tagliato.\n",
    "\n",
    "### **6.5 Workflow Finale: Dal Tuning alla Produzione**\n",
    "\n",
    "Una volta che `study.best_params` ti restituisce il dizionario vincente, non hai finito.\n",
    "\n",
    "1.  **Prendi i migliori parametri.**\n",
    "2.  **Applica la strategia \"Low Rate\" (Shrinkage):**\n",
    "      * Prendi il `learning_rate` suggerito da Optuna e dividilo (es. per 2 o per 5).\n",
    "      * Aumenta `n_estimators` per compensare.\n",
    "3.  **Rialilena sul dataset completo:** Ora usa tutto il dataset (Train + Validation) per addestrare il modello finale che andrà in produzione, usando il numero di step ottimale trovato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f1c3504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Caricamento Dati ---\n",
      "Dataset Shape: (569, 30)\n",
      "Scale Pos Weight calcolato: 0.60\n",
      "\n",
      "--- Training Modello Baseline (Default) ---\n",
      "Baseline AUC Score: 0.9901\n",
      "\n",
      "--- Inizio Tuning con Optuna ---\n",
      "Migliori parametri trovati:\n",
      "{'max_depth': 5, 'min_child_weight': 2, 'gamma': 1.3739595360819293e-07, 'subsample': 0.6625543739709935, 'colsample_bytree': 0.8955340177076467, 'lambda': 1.0737165712542365, 'alpha': 0.0015620425834124514, 'learning_rate': 0.267058086978701}\n",
      "\n",
      "--- Training Modello Finale Ottimizzato ---\n",
      "\n",
      "--- RISULTATI FINALI SUL TEST SET ---\n",
      "1. AUC Score Baseline:    0.99008\n",
      "2. AUC Score Ottimizzato: 0.99140\n",
      "------------------------------\n",
      "Miglioramento AUC:        +0.00132\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "# Silenziamo alcuni warning di Optuna per pulizia\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. Preparazione Dati\n",
    "# ==========================================\n",
    "print(\"--- Caricamento Dati ---\")\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split 1: Togliamo il 20% dei dati per il TEST finale (Dati mai visti né da XGBoost né da Optuna)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Calcolo scale_pos_weight per bilanciamento (best practice)\n",
    "# Anche se questo dataset è abbastanza bilanciato, lo calcoliamo per rigore professionale\n",
    "num_neg = np.sum(y_train_full == 0)\n",
    "num_pos = np.sum(y_train_full == 1)\n",
    "scale_pos_weight = num_neg / num_pos\n",
    "\n",
    "print(f\"Dataset Shape: {X.shape}\")\n",
    "print(f\"Scale Pos Weight calcolato: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Baseline (Senza Tuning)\n",
    "# ==========================================\n",
    "print(\"\\n--- Training Modello Baseline (Default) ---\")\n",
    "dtrain_full = xgb.DMatrix(X_train_full, label=y_train_full)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Parametri standard\n",
    "params_base = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'tree_method': 'hist', # Più veloce\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "model_base = xgb.train(params_base, dtrain_full, num_boost_round=100)\n",
    "preds_base = model_base.predict(dtest)\n",
    "auc_base = roc_auc_score(y_test, preds_base)\n",
    "print(f\"Baseline AUC Score: {auc_base:.4f}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. Ottimizzazione con Optuna\n",
    "# ==========================================\n",
    "print(\"\\n--- Inizio Tuning con Optuna ---\")\n",
    "\n",
    "def objective(trial):\n",
    "    # A. Split interno per Optuna (Train vs Validation)\n",
    "    # Optuna usa questo valid set per decidere se i parametri sono buoni\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    # B. Definizione Spazio Iperparametri\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss', # Metrica monitorata per il pruning\n",
    "        'tree_method': 'hist',\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        \n",
    "        # --- Parametri da Ottimizzare ---\n",
    "        # 1. Struttura\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        \n",
    "        # 2. Campionamento\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        \n",
    "        # 3. Regolarizzazione\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "        \n",
    "        # 4. Learning\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "    }\n",
    "\n",
    "    # Callback per il Pruning (Interrompe i trial scarsi)\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-logloss\")\n",
    "\n",
    "    # Training del trial\n",
    "    model = xgb.train(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=1000, # Alto numero teorico\n",
    "        evals=[(dvalid, \"validation\")],\n",
    "        early_stopping_rounds=50, # Stop se non migliora\n",
    "        callbacks=[pruning_callback],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Predizione sul validation set interno\n",
    "    preds = model.predict(dvalid)\n",
    "    # Ottimizziamo la LogLoss (più bassa è meglio)\n",
    "    # Nota: Optuna minimizza per default, log_loss è perfetta\n",
    "    loss = classification_report(y_valid, preds > 0.5, output_dict=True)['accuracy'] # Trucco: ottimizziamo accuracy invertita o logloss diretta\n",
    "    \n",
    "    # Per semplicità in questo esempio usiamo logloss diretta\n",
    "    # Importante: model.predict restituisce probabilità\n",
    "    from sklearn.metrics import log_loss\n",
    "    return log_loss(y_valid, preds)\n",
    "\n",
    "# Creazione Studio\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, timeout=600) # 50 tentativi o 10 minuti\n",
    "\n",
    "print(\"Migliori parametri trovati:\")\n",
    "print(study.best_params)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. Training Modello Finale (Best Params)\n",
    "# ==========================================\n",
    "print(\"\\n--- Training Modello Finale Ottimizzato ---\")\n",
    "\n",
    "# Recuperiamo i migliori parametri\n",
    "best_params = study.best_params\n",
    "\n",
    "# Aggiungiamo i parametri fissi necessari\n",
    "best_params['objective'] = 'binary:logistic'\n",
    "best_params['eval_metric'] = 'logloss'\n",
    "best_params['tree_method'] = 'hist'\n",
    "best_params['scale_pos_weight'] = scale_pos_weight\n",
    "\n",
    "# TECNICA DEL \"LOW LEARNING RATE\" (Shrinkage)\n",
    "# Riduciamo il learning rate trovato e aumentiamo gli alberi per precisione massima\n",
    "best_params['learning_rate'] = best_params['learning_rate'] / 2 \n",
    "num_boost_round_final = 2000 \n",
    "\n",
    "# Addestriamo su TUTTO il set di training (Train + Valid interno di Optuna)\n",
    "model_opt = xgb.train(\n",
    "    best_params, \n",
    "    dtrain_full, \n",
    "    num_boost_round=num_boost_round_final,\n",
    "    evals=[(dtest, \"test\")], # Usiamo il test set solo per early stopping finale\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 5. Confronto Finale\n",
    "# ==========================================\n",
    "print(\"\\n--- RISULTATI FINALI SUL TEST SET ---\")\n",
    "\n",
    "# Predizioni\n",
    "preds_opt = model_opt.predict(dtest)\n",
    "\n",
    "# Metriche\n",
    "auc_opt = roc_auc_score(y_test, preds_opt)\n",
    "acc_base = accuracy_score(y_test, preds_base > 0.5)\n",
    "acc_opt = accuracy_score(y_test, preds_opt > 0.5)\n",
    "\n",
    "print(f\"1. AUC Score Baseline:    {auc_base:.5f}\")\n",
    "print(f\"2. AUC Score Ottimizzato: {auc_opt:.5f}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Miglioramento AUC:        {auc_opt - auc_base:+.5f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Visualizzazione importanza iperparametri (se in notebook)\n",
    "# optuna.visualization.plot_param_importances(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2566963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Caricamento California Housing Dataset ---\n",
      "Dataset Shape: (20640, 8)\n",
      "Target medio: 2.07 (centinaia di k$)\n",
      "\n",
      "--- Training Modello Baseline (Default) ---\n",
      "Baseline RMSE: 0.4718\n",
      "\n",
      "--- Inizio Tuning con Optuna ---\n",
      "\n",
      "Miglior RMSE trovato da Optuna (Validation): 0.4618\n",
      "Migliori parametri: {'max_depth': 10, 'min_child_weight': 16, 'gamma': 0.0012818711788339556, 'subsample': 0.9191051088489676, 'colsample_bytree': 0.8034712852048401, 'lambda': 4.907568291151285e-07, 'alpha': 4.251394710757856e-05, 'learning_rate': 0.013635246924182255}\n",
      "\n",
      "--- Training Modello Finale Ottimizzato ---\n",
      "\n",
      "--- CONFRONTO FINALE ---\n",
      "RMSE Baseline:    0.4718\n",
      "RMSE Ottimizzato: 0.4341\n",
      "--> Miglioramento Errore: 0.0377 (Minore è meglio)\n",
      "------------------------------\n",
      "R2 Score Baseline:    0.8301\n",
      "R2 Score Ottimizzato: 0.8562\n",
      "--> Varianza Spiegata Extra: +2.61%\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Setup pulizia output\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. Caricamento Dati (California Housing)\n",
    "# ==========================================\n",
    "print(\"--- Caricamento California Housing Dataset ---\")\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# X = 8 feature (MedInc, HouseAge, AveRooms, etc.)\n",
    "# y = Prezzo mediano (in centinaia di migliaia di $)\n",
    "\n",
    "# Split Train/Test (Test set intoccabile fino alla fine)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dataset Shape: {X.shape}\")\n",
    "print(f\"Target medio: {np.mean(y):.2f} (centinaia di k$)\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Baseline (Senza Tuning)\n",
    "# ==========================================\n",
    "print(\"\\n--- Training Modello Baseline (Default) ---\")\n",
    "dtrain_full = xgb.DMatrix(X_train_full, label=y_train_full)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Parametri standard di default\n",
    "params_base = {\n",
    "    'objective': 'reg:squarederror', # Regressione\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "model_base = xgb.train(params_base, dtrain_full, num_boost_round=100)\n",
    "preds_base = model_base.predict(dtest)\n",
    "\n",
    "# Calcolo RMSE Baseline\n",
    "rmse_base = np.sqrt(mean_squared_error(y_test, preds_base))\n",
    "print(f\"Baseline RMSE: {rmse_base:.4f}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. Ottimizzazione con Optuna\n",
    "# ==========================================\n",
    "print(\"\\n--- Inizio Tuning con Optuna ---\")\n",
    "\n",
    "def objective(trial):\n",
    "    # Split interno per validazione durante il tuning\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    # Spazio di ricerca\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse', # Metrica per early stopping e pruning\n",
    "        'tree_method': 'hist',\n",
    "        \n",
    "        # --- Struttura ---\n",
    "        # California ha interazioni complesse, permettiamo alberi più profondi\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "        # Importante per evitare overfitting su outlier di prezzo\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        \n",
    "        # --- Randomness ---\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        \n",
    "        # --- Regolarizzazione ---\n",
    "        # Fondamentale nella regressione per non inseguire i prezzi estremi\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 20.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 20.0, log=True),\n",
    "        \n",
    "        # --- Learning ---\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "    }\n",
    "\n",
    "    # Pruning basato su RMSE\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\")\n",
    "\n",
    "    model = xgb.train(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dvalid, \"validation\")],\n",
    "        early_stopping_rounds=50,\n",
    "        callbacks=[pruning_callback],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Optuna deve minimizzare RMSE\n",
    "    preds = model.predict(dvalid)\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, preds))\n",
    "    return rmse\n",
    "\n",
    "# Creazione Studio (Minimizzare RMSE)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, timeout=600)\n",
    "\n",
    "print(f\"\\nMiglior RMSE trovato da Optuna (Validation): {study.best_value:.4f}\")\n",
    "print(\"Migliori parametri:\", study.best_params)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. Training Modello Finale (Best Params + Shrinkage)\n",
    "# ==========================================\n",
    "print(\"\\n--- Training Modello Finale Ottimizzato ---\")\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params['objective'] = 'reg:squarederror'\n",
    "best_params['eval_metric'] = 'rmse'\n",
    "best_params['tree_method'] = 'hist'\n",
    "\n",
    "# STRATEGIA \"SHRINKAGE\" (Raffiniamo la discesa del gradiente)\n",
    "# Abbassiamo il learning rate trovato per maggiore precisione finale\n",
    "best_params['learning_rate'] = best_params['learning_rate'] * 0.5 \n",
    "# Aumentiamo gli alberi di conseguenza (safety buffer alto, early stopping gestirà il resto)\n",
    "num_round_final = 5000 \n",
    "\n",
    "model_opt = xgb.train(\n",
    "    best_params, \n",
    "    dtrain_full, \n",
    "    num_boost_round=num_round_final,\n",
    "    evals=[(dtest, \"test\")], # Test set usato SOLO per fermare il training al punto giusto\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 5. Analisi Risultati\n",
    "# ==========================================\n",
    "print(\"\\n--- CONFRONTO FINALE ---\")\n",
    "\n",
    "preds_opt = model_opt.predict(dtest)\n",
    "rmse_opt = np.sqrt(mean_squared_error(y_test, preds_opt))\n",
    "\n",
    "# R2 Score (coefficiente di determinazione)\n",
    "r2_base = r2_score(y_test, preds_base)\n",
    "r2_opt = r2_score(y_test, preds_opt)\n",
    "\n",
    "print(f\"RMSE Baseline:    {rmse_base:.4f}\")\n",
    "print(f\"RMSE Ottimizzato: {rmse_opt:.4f}\")\n",
    "print(f\"--> Miglioramento Errore: {rmse_base - rmse_opt:.4f} (Minore è meglio)\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"R2 Score Baseline:    {r2_base:.4f}\")\n",
    "print(f\"R2 Score Ottimizzato: {r2_opt:.4f}\")\n",
    "print(f\"--> Varianza Spiegata Extra: +{(r2_opt - r2_base)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234b187",
   "metadata": {},
   "source": [
    "### **Pro-Tip: Monotonic Constraints su questo Dataset**\n",
    "\n",
    "Se volessi rendere questo modello \"bullet-proof\" per un'agenzia immobiliare reale, dovresti aggiungere un vincolo al parametro `AveRooms` (Numero medio di stanze).\n",
    "\n",
    "Nella realtà, *a parità di altre condizioni* (stessa zona, stessa età), una casa con più stanze vale di più. Se il modello imparasse il contrario (magari per rumore statistico), sarebbe un errore grave.\n",
    "\n",
    "Potresti aggiungere questo nel `param` finale:\n",
    "\n",
    "```python\n",
    "# Supponendo che 'AveRooms' sia la colonna indice 2\n",
    "# 1 = crescente, 0 = nessun vincolo\n",
    "# Imponiamo che più stanze = prezzo più alto (o uguale)\n",
    "constraints = (0, 0, 1, 0, 0, 0, 0, 0) \n",
    "best_params['monotone_constraints'] = constraints\n",
    "```\n",
    "\n",
    "Questo peggiorerebbe leggermente l'RMSE matematico, ma renderebbe il modello infinitamente più affidabile nel mondo reale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12261d8",
   "metadata": {},
   "source": [
    "# **Ensemble a Media Ponderata (Weighted Blending)**.\n",
    "\n",
    "Questo approccio è spesso superiore ai singoli modelli perché riduce la varianza complessiva e sfrutta i diversi bias induttivi dei tre algoritmi di Gradient Boosting.\n",
    "\n",
    "\n",
    "### Prerequisiti\n",
    "\n",
    "Assicurati di avere le librerie installate:\n",
    "`pip install xgboost catboost lightgbm scikit-learn pandas numpy`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c28488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dataset California Housing...\n",
      "Addestramento modelli in corso...\n",
      "XGBoost completato.\n",
      "CatBoost completato.\n",
      "LightGBM completato.\n",
      "\n",
      "--- Risultati (RMSE più basso è migliore) ---\n",
      "XGBoost         | RMSE: 0.4481\n",
      "CatBoost        | RMSE: 0.4520\n",
      "LightGBM        | RMSE: 0.4336\n",
      "-----------------------------------\n",
      "Ensemble (Blend) | RMSE: 0.4346\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Import dei Regressori\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# 1. Caricamento e Preparazione Dati\n",
    "# ---------------------------------------------------------\n",
    "print(\"Caricamento dataset California Housing...\")\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Split 80/20 standard\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Definizione dei Modelli (Base Parameters)\n",
    "# ---------------------------------------------------------\n",
    "# Nota: Qui usiamo parametri \"sensati\" ma statici. \n",
    "# In produzione, questi verrebbero iniettati dopo uno studio Optuna.\n",
    "\n",
    "params_common = {'random_state': 42}\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    n_jobs=-1,\n",
    "    **params_common\n",
    ")\n",
    "\n",
    "# CatBoost (verbose=0 per silenziare l'output durante il training)\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    verbose=0,\n",
    "    allow_writing_files=False,\n",
    "    **params_common\n",
    ")\n",
    "\n",
    "# LightGBM\n",
    "lgbm_model = LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1, # Silenzia i warning\n",
    "    **params_common\n",
    ")\n",
    "\n",
    "# 3. Training\n",
    "# ---------------------------------------------------------\n",
    "print(\"Addestramento modelli in corso...\")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(f\"XGBoost completato.\")\n",
    "\n",
    "cat_model.fit(X_train, y_train)\n",
    "print(f\"CatBoost completato.\")\n",
    "\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "print(f\"LightGBM completato.\")\n",
    "\n",
    "# 4. Predizione e Ensemble (Weighted Blending)\n",
    "# ---------------------------------------------------------\n",
    "# Generiamo le predizioni individuali sul Test Set\n",
    "preds_xgb = xgb_model.predict(X_test)\n",
    "preds_cat = cat_model.predict(X_test)\n",
    "preds_lgbm = lgbm_model.predict(X_test)\n",
    "\n",
    "# Definizione dei pesi specificati\n",
    "w_xgb = 0.4\n",
    "w_cat = 0.2\n",
    "w_lgbm = 0.4\n",
    "\n",
    "# Calcolo della predizione combinata\n",
    "# Formula: y_pred = (0.4 * XGB) + (0.2 * CAT) + (0.4 * LGBM)\n",
    "preds_ensemble = (preds_xgb * w_xgb) + (preds_cat * w_cat) + (preds_lgbm * w_lgbm)\n",
    "\n",
    "# 5. Valutazione e Confronto\n",
    "# ---------------------------------------------------------\n",
    "def evaluate(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f\"{model_name:<15} | RMSE: {rmse:.4f}\")\n",
    "\n",
    "print(\"\\n--- Risultati (RMSE più basso è migliore) ---\")\n",
    "evaluate(y_test, preds_xgb, \"XGBoost\")\n",
    "evaluate(y_test, preds_cat, \"CatBoost\")\n",
    "evaluate(y_test, preds_lgbm, \"LightGBM\")\n",
    "print(\"-\" * 35)\n",
    "evaluate(y_test, preds_ensemble, \"Ensemble (Blend)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ed481",
   "metadata": {},
   "source": [
    "### Analisi Tecnica della Soluzione\n",
    "\n",
    "In questo esempio abbiamo applicato una tecnica di **Averaging Ensemble**. La logica matematica dietro la combinazione è lineare:\n",
    "\n",
    "$$\\hat{y}_{ensemble} = w_1 \\cdot \\hat{y}_{xgb} + w_2 \\cdot \\hat{y}_{cat} + w_3 \\cdot \\hat{y}_{lgbm}$$\n",
    "\n",
    "Dove la somma dei pesi è unitaria: $\\sum w_i = 1$.\n",
    "\n",
    "**Perché questa combinazione funziona?**\n",
    "\n",
    "  * **XGBoost (0.4):** Solitamente offre un ottimo bilanciamento tra bias e varianza ed è robusto sugli outlier. Gli diamo un peso alto.\n",
    "  * **LightGBM (0.4):** Spesso molto aggressivo e veloce, tende a performare in modo simile a XGBoost ma costruisce gli alberi in modo diverso (Leaf-wise vs Level-wise), catturando pattern leggermente differenti.\n",
    "  * **CatBoost (0.2):** Anche se eccelle con feature categoriche (qui non ne abbiamo di complesse nel dataset California), è ottimo per regolarizzare. Il peso minore (0.2) agisce come un \"correttore\" per evitare che i due modelli principali facciano overfitting sugli stessi errori."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa6ae6",
   "metadata": {},
   "source": [
    "Se il *Weighted Blending* è l'approccio \"artigianale\" (dove noi imponiamo la nostra conoscenza del dominio tramite i pesi 0.4, 0.2, etc.), lo **Stacking** è l'approccio \"data-driven\".\n",
    "\n",
    "Qui non definiamo noi i pesi. Addestriamo un **Meta-Modello (Level 1)** che impara dagli errori dei **Base Models (Level 0)**.\n",
    "\n",
    "### Concetto Chiave: Il Meta-Learner\n",
    "\n",
    "Invece di una media pesata lineare fissa:\n",
    "$$\\hat{y} = 0.4 \\cdot X + 0.2 \\cdot C + 0.4 \\cdot L$$\n",
    "\n",
    "Lo Stacking addestra un modello (spesso una regressione lineare regolarizzata come la `Ridge` o `Lasso`) che prende in input le predizioni dei modelli base:\n",
    "$$\\hat{y} = f_{meta}(\\hat{y}_{xgb}, \\hat{y}_{cat}, \\hat{y}_{lgbm})$$\n",
    "\n",
    "Ecco come implementarlo con `sklearn.ensemble.StackingRegressor`, mantenendo i parametri ottimizzati.\n",
    "\n",
    "### Il Codice Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d393687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento e split dati...\n",
      "2. Avvio Ottimizzazione Iperparametri (20 trial per modello)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1c8138d7504b67a9a5f5179b56167f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8700d87b2a904b8bbe3c6554f1a0963f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112d80cad8b14c3faef60fcb3c72a832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best RMSE Val -> XGB: 0.4547 | CAT: 0.4473 | LGBM: 0.4561\n",
      "\n",
      "3. Configurazione Stacking Regressor con i migliori parametri...\n",
      "Addestramento Stacking (questo passaggio ri-addestra i base models su 5 fold)...\n",
      "\n",
      "4. Valutazione sul Test Set (Hold-out)...\n",
      "----------------------------------------\n",
      "STACKING FINAL RMSE: 0.4270\n",
      "STACKING FINAL R2:   0.8609\n",
      "----------------------------------------\n",
      "Pesi assegnati dal Meta-Modello (Ridge):\n",
      "Intercept: -0.0093\n",
      "XGBoost   : 0.2467\n",
      "CatBoost  : 0.5345\n",
      "LightGBM  : 0.2234\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import logging\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Modelli\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Stacking\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Configurazione Silenziosa\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. PREPARAZIONE DATI\n",
    "# ---------------------------------------------------------\n",
    "print(\"1. Caricamento e split dati...\")\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Split Principale: 80% Train (per Optuna + Stacking), 20% Test (Hold-out finale)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split Secondario: Solo per Optuna (per non overfittare sul train set intero durante il tuning)\n",
    "X_opt_train, X_opt_val, y_opt_train, y_opt_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. OTTIMIZZAZIONE OPTUNA (LIGHT)\n",
    "# ---------------------------------------------------------\n",
    "print(\"2. Avvio Ottimizzazione Iperparametri (20 trial per modello)...\")\n",
    "\n",
    "# --- XGBoost ---\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train)\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=20, show_progress_bar=True)\n",
    "best_xgb_params = study_xgb.best_params\n",
    "best_xgb_params.update({'n_jobs': -1, 'random_state': 42}) # Reinseriamo params statici\n",
    "\n",
    "# --- CatBoost ---\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 500, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'verbose': 0,\n",
    "        'allow_writing_files': False,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train)\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "study_cat = optuna.create_study(direction='minimize')\n",
    "study_cat.optimize(objective_cat, n_trials=20, show_progress_bar=True)\n",
    "best_cat_params = study_cat.best_params\n",
    "best_cat_params.update({'verbose': 0, 'allow_writing_files': False, 'random_state': 42})\n",
    "\n",
    "# --- LightGBM ---\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 15),\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train)\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=20, show_progress_bar=True)\n",
    "best_lgbm_params = study_lgbm.best_params\n",
    "best_lgbm_params.update({'n_jobs': -1, 'verbose': -1, 'random_state': 42})\n",
    "\n",
    "print(f\"\\nBest RMSE Val -> XGB: {study_xgb.best_value:.4f} | CAT: {study_cat.best_value:.4f} | LGBM: {study_lgbm.best_value:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. COSTRUZIONE E TRAINING STACKING REGRESSOR\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n3. Configurazione Stacking Regressor con i migliori parametri...\")\n",
    "\n",
    "# Definizione Level 0 (Base Models)\n",
    "estimators = [\n",
    "    ('xgb', XGBRegressor(**best_xgb_params)),\n",
    "    ('cat', CatBoostRegressor(**best_cat_params)),\n",
    "    ('lgbm', LGBMRegressor(**best_lgbm_params))\n",
    "]\n",
    "\n",
    "# Definizione Level 1 (Meta Model)\n",
    "# RidgeCV trova automaticamente la regolarizzazione (alpha) migliore\n",
    "meta_learner = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,            # 5-Fold per generare le previsioni out-of-fold robuste\n",
    "    n_jobs=-1,\n",
    "    passthrough=False \n",
    ")\n",
    "\n",
    "print(\"Addestramento Stacking (questo passaggio ri-addestra i base models su 5 fold)...\")\n",
    "stacking_regressor.fit(X_train_full, y_train_full)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. VALUTAZIONE FINALE\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n4. Valutazione sul Test Set (Hold-out)...\")\n",
    "\n",
    "# Predizione\n",
    "preds = stacking_regressor.predict(X_test)\n",
    "\n",
    "# Metriche\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"STACKING FINAL RMSE: {rmse:.4f}\")\n",
    "print(f\"STACKING FINAL R2:   {r2:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Ispezione Pesi Meta-Learner\n",
    "print(\"Pesi assegnati dal Meta-Modello (Ridge):\")\n",
    "print(f\"Intercept: {stacking_regressor.final_estimator_.intercept_:.4f}\")\n",
    "model_names = ['XGBoost', 'CatBoost', 'LightGBM']\n",
    "coeffs = stacking_regressor.final_estimator_.coef_\n",
    "\n",
    "for name, coef in zip(model_names, coeffs):\n",
    "    print(f\"{name:<10}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e78d7",
   "metadata": {},
   "source": [
    "### Integriamo un altro modello di ml internamente e facciamo un leggerisisimo feature engineering e outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01cd8971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Data Loading & Statistical Cleaning...\n",
      "Dataset pulito con IQR: 15203 righe (Rimossi 4445 outlier)\n",
      "2. Creazione Feature (Distanze, Rapporti, Cluster)...\n",
      "    -> Generazione Cluster Geografici (KMeans)...\n",
      "\n",
      "3. Tuning Iperparametri...\n",
      "Best RMSE Val -> XGB: 0.3688 | CAT: 0.3684 | LGBM: 0.3734\n",
      "\n",
      "4. Addestramento Stacking Ensemble (CV=5)...\n",
      "\n",
      "5. Valutazione Finale, Coefficienti Stacking e Analisi Importanza Feature...\n",
      "\n",
      "--- RISULTATI FINALI STACKING ENSEMBLE ---\n",
      "Final RMSE (Test Set): 0.3619\n",
      "Final R-squared (Test Set): 0.8406\n",
      "------------------------------------------\n",
      "\n",
      "--- COEFFICIENTI DEL MODELLO DI STACKING (RIDGE CV) ---\n",
      "Coefficiente XGB : 0.2486\n",
      "Coefficiente CAT : 0.5102\n",
      "Coefficiente LGBM: 0.2590\n",
      "-------------------------------------------------------\n",
      "\n",
      "Top 5 Feature più importanti (Basate su XGBoost Base Estimator):\n",
      "Geo_Cluster         0.462401\n",
      "Rooms_per_Person    0.087847\n",
      "Dist_Min_Metro      0.079249\n",
      "MedInc              0.067518\n",
      "Dist_SF             0.048889\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import warnings\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Modelli\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Stacking\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Configurazione Silenziosa\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. CARICAMENTO E PULIZIA AVANZATA (IQR)\n",
    "# ---------------------------------------------------------\n",
    "print(\"1. Data Loading & Statistical Cleaning...\")\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"MedHouseVal\")\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Rimuoviamo il cap artificiale del target (valori >= 5.0)\n",
    "df = df[df['MedHouseVal'] < 5.0]\n",
    "\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    \"\"\"\n",
    "    Rimuove gli outlier usando la regola dell'IQR (Interquartile Range).\n",
    "    Mantiene solo i dati tra Q1 - 1.5*IQR e Q3 + 1.5*IQR.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    indices_to_drop = []\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.30)\n",
    "        Q3 = df_clean[col].quantile(0.70)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identifichiamo gli indici da rimuovere per questa colonna\n",
    "        outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)].index\n",
    "        indices_to_drop.extend(outliers)\n",
    "    \n",
    "    # Rimuoviamo i duplicati degli indici e facciamo il drop\n",
    "    indices_to_drop = list(set(indices_to_drop))\n",
    "    df_clean = df_clean.drop(indices_to_drop)\n",
    "    return df_clean\n",
    "\n",
    "# Applichiamo IQR solo su feature \"fisiche\" soggette a errori di input o anomalie estreme\n",
    "cols_to_clean = ['AveRooms', 'AveBedrms', 'AveOccup', 'MedInc']\n",
    "original_len = len(df)\n",
    "df = remove_outliers_iqr(df, cols_to_clean)\n",
    "print(f\"Dataset pulito con IQR: {len(df)} righe (Rimossi {original_len - len(df)} outlier)\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. FEATURE ENGINEERING AVANZATA\n",
    "# ---------------------------------------------------------\n",
    "print(\"2. Creazione Feature (Distanze, Rapporti, Cluster)...\")\n",
    "\n",
    "# A. Feature basate su Domain Knowledge\n",
    "df['Bedrms_per_Room'] = df['AveBedrms'] / df['AveRooms']\n",
    "df['Rooms_per_Person'] = df['AveRooms'] / df['AveOccup']\n",
    "# Interazione Reddito * Stanze (Capacità di spesa per spazio)\n",
    "df['Wealth_Capacity'] = df['MedInc'] * df['AveRooms']\n",
    "\n",
    "# B. Feature Geospaziali: Distanza dai centri economici\n",
    "# Coordinate approssimative (Lat, Lon)\n",
    "sf_coords = (37.7749, -122.4194)\n",
    "la_coords = (34.0522, -118.2437)\n",
    "\n",
    "def dist_calc(row, city_coords):\n",
    "    # Distanza Euclidea semplificata (sufficiente per ML su scala locale)\n",
    "    return np.sqrt((row['Latitude'] - city_coords[0])**2 + (row['Longitude'] - city_coords[1])**2)\n",
    "\n",
    "df['Dist_SF'] = df.apply(lambda x: dist_calc(x, sf_coords), axis=1)\n",
    "df['Dist_LA'] = df.apply(lambda x: dist_calc(x, la_coords), axis=1)\n",
    "# Feature: Distanza minima dal centro metropolitano più vicino\n",
    "df['Dist_Min_Metro'] = df[['Dist_SF', 'Dist_LA']].min(axis=1)\n",
    "\n",
    "# Separazione X e y\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Split Train/Test\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# C. KMeans Clustering (Gestito correttamente per evitare Leakage)\n",
    "# Fit solo su Train, Transform su Train e Test\n",
    "print(\"    -> Generazione Cluster Geografici (KMeans)...\")\n",
    "scaler_geo = StandardScaler()\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "\n",
    "# Prendiamo solo lat/long\n",
    "train_geo = X_train_full[['Latitude', 'Longitude']]\n",
    "test_geo = X_test[['Latitude', 'Longitude']]\n",
    "\n",
    "# Scaliamo (KMeans è sensibile alla scala)\n",
    "train_geo_scaled = scaler_geo.fit_transform(train_geo)\n",
    "test_geo_scaled = scaler_geo.transform(test_geo)\n",
    "\n",
    "# Creiamo la feature cluster\n",
    "X_train_full['Geo_Cluster'] = kmeans.fit_predict(train_geo_scaled)\n",
    "X_test['Geo_Cluster'] = kmeans.predict(test_geo_scaled)\n",
    "\n",
    "# Assicuriamoci che Geo_Cluster sia trattata come categorica o intera\n",
    "X_train_full['Geo_Cluster'] = X_train_full['Geo_Cluster'].astype(int)\n",
    "X_test['Geo_Cluster'] = X_test['Geo_Cluster'].astype(int)\n",
    "\n",
    "\n",
    "# Split per Optuna (dal Train set pulito)\n",
    "X_opt_train, X_opt_val, y_opt_train, y_opt_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. OTTIMIZZAZIONE OPTUNA\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n3. Tuning Iperparametri...\")\n",
    "\n",
    "# --- XGBoost ---\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 600, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.08),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, model.predict(X_opt_val)))\n",
    "\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=15) # 15 trial per velocità\n",
    "best_xgb = study_xgb.best_params\n",
    "best_xgb.update({'n_jobs': -1, 'random_state': 42})\n",
    "\n",
    "# --- CatBoost ---\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 600, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.08),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'verbose': 0,\n",
    "        'allow_writing_files': False,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train, cat_features=['Geo_Cluster'])\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, model.predict(X_opt_val)))\n",
    "\n",
    "study_cat = optuna.create_study(direction='minimize')\n",
    "study_cat.optimize(objective_cat, n_trials=15)\n",
    "best_cat = study_cat.best_params\n",
    "best_cat.update({'verbose': 0, 'allow_writing_files': False, 'random_state': 42})\n",
    "\n",
    "# --- LightGBM ---\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 600, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.08),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = LGBMRegressor(**params)\n",
    "    # LightGBM gestisce le categorie (Geo_Cluster) se specificate\n",
    "    cat_features = ['Geo_Cluster']\n",
    "    X_train_lgbm = X_opt_train.copy()\n",
    "    X_val_lgbm = X_opt_val.copy()\n",
    "    X_train_lgbm[cat_features] = X_train_lgbm[cat_features].astype('category')\n",
    "    X_val_lgbm[cat_features] = X_val_lgbm[cat_features].astype('category')\n",
    "    \n",
    "    model.fit(X_train_lgbm, y_opt_train, categorical_feature=cat_features)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, model.predict(X_val_lgbm)))\n",
    "\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=15)\n",
    "best_lgbm = study_lgbm.best_params\n",
    "best_lgbm.update({'n_jobs': -1, 'verbose': -1, 'random_state': 42})\n",
    "\n",
    "print(f\"Best RMSE Val -> XGB: {study_xgb.best_value:.4f} | CAT: {study_cat.best_value:.4f} | LGBM: {study_lgbm.best_value:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. STACKING REGRESSOR\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n4. Addestramento Stacking Ensemble (CV=5)...\")\n",
    "\n",
    "# Prepariamo i dati per Stacking (gestione delle feature categoriali per lgbm e cat)\n",
    "X_train_stack = X_train_full.copy()\n",
    "X_test_stack = X_test.copy()\n",
    "cat_features = ['Geo_Cluster']\n",
    "X_train_stack[cat_features] = X_train_stack[cat_features].astype('category')\n",
    "X_test_stack[cat_features] = X_test_stack[cat_features].astype('category')\n",
    "\n",
    "estimators = [\n",
    "    ('xgb', XGBRegressor(**best_xgb, enable_categorical=True)),\n",
    "    ('cat', CatBoostRegressor(**best_cat, cat_features=cat_features)),\n",
    "    ('lgbm', LGBMRegressor(**best_lgbm, categorical_feature=cat_features))\n",
    "]\n",
    "\n",
    "# RidgeCV usa la Generalized Cross-Validation per trovare l'alpha ottimale in modo efficiente\n",
    "meta_learner = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "\n",
    "stacking = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False \n",
    ")\n",
    "\n",
    "# Addestramento finale su tutto il set di training\n",
    "stacking.fit(X_train_stack, y_train_full)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. RISULTATI FINALI, COEFFICIENTI e FEATURE IMPORTANCE\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n5. Valutazione Finale, Coefficienti Stacking e Analisi Importanza Feature...\")\n",
    "\n",
    "# 5.1. Calcolo e Stampa delle Metriche Finali\n",
    "y_pred = stacking.predict(X_test_stack)\n",
    "\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "final_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n--- RISULTATI FINALI STACKING ENSEMBLE ---\")\n",
    "print(f\"Final RMSE (Test Set): {final_rmse:.4f}\")\n",
    "print(f\"Final R-squared (Test Set): {final_r2:.4f}\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "\n",
    "# 5.2. Stampa dei Coefficienti del Meta-Learner (RidgeCV)\n",
    "meta_coef = stacking.final_estimator_.coef_\n",
    "base_names = [name for name, _ in stacking.estimators]\n",
    "\n",
    "print(\"\\n--- COEFFICIENTI DEL MODELLO DI STACKING (RIDGE CV) ---\")\n",
    "for name, coef in zip(base_names, meta_coef):\n",
    "    print(f\"Coefficiente {name.upper():<4}: {coef:.4f}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "# Nota: La somma dei coefficienti dovrebbe essere vicina a 1, indicando una media ponderata.\n",
    "\n",
    "\n",
    "# 5.3. Analisi Importanza Feature\n",
    "xgb_temp = stacking.estimators_[0]\n",
    "feature_names = X_train_full.columns \n",
    "\n",
    "importances = pd.Series(xgb_temp.feature_importances_, index=feature_names)\n",
    "\n",
    "print(\"\\nTop 5 Feature più importanti (Basate su XGBoost Base Estimator):\")\n",
    "print(importances.sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fccb5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento Dati...\n",
      "2. Feature Engineering...\n",
      "   -> Totale Feature create: 111\n",
      "3. Selezione Feature...\n",
      "   -> Feature Mantenute: 34\n",
      "\n",
      "4. Stacking Ensemble...\n",
      "\n",
      "==========================================\n",
      " FINAL TEST SET RESULTS (STACKING)\n",
      "==========================================\n",
      " RMSE: 0.44003\n",
      " R^2 : 0.85224\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import warnings\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. CARICAMENTO DATI (outlier mantenuti)\n",
    "# ---------------------------------------------------------\n",
    "print(\"1. Caricamento Dati...\")\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"MedHouseVal\")  # target originale, NO log\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. FEATURE ENGINEERING MIRATO (riduci rumore)\n",
    "# ---------------------------------------------------------\n",
    "print(\"2. Feature Engineering...\")\n",
    "\n",
    "# Geo features\n",
    "sf_coords = (37.7749, -122.4194)\n",
    "la_coords = (34.0522, -118.2437)\n",
    "X['Dist_SF'] = np.sqrt((X['Latitude'] - sf_coords[0])**2 + (X['Longitude'] - sf_coords[1])**2)\n",
    "X['Dist_LA'] = np.sqrt((X['Latitude'] - la_coords[0])**2 + (X['Longitude'] - la_coords[1])**2)\n",
    "\n",
    "# Cluster geografici\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "geo_scaled = StandardScaler().fit_transform(X[['Latitude', 'Longitude']])\n",
    "X['Geo_Cluster'] = kmeans.fit_predict(geo_scaled)\n",
    "\n",
    "# Feature robuste e poche ma buone\n",
    "X['Households'] = (X['Population'] / X['AveOccup']).clip(lower=1)\n",
    "X['RoomsPerHousehold'] = X['AveRooms'] / X['Households']\n",
    "X['PopulationPerHousehold'] = X['Population'] / X['Households']\n",
    "X['BedroomsPerRoom'] = X['AveBedrms'] / (X['AveRooms'] + 1e-5)\n",
    "\n",
    "# Log sulle feature skewed\n",
    "X['LOG_MedInc'] = np.log1p(X['MedInc'])\n",
    "X['LOG_Population'] = np.log1p(X['Population'])\n",
    "X['LOG_AveRooms'] = np.log1p(X['AveRooms'])\n",
    "X['IncomePerOccupant'] = X['MedInc'] / (X['AveOccup'] + 1e-5)\n",
    "\n",
    "# Interazioni mirate (solo poche utili)\n",
    "X['MedInc_x_HouseAge'] = X['MedInc'] * X['HouseAge']\n",
    "X['MedInc_div_AveOccup'] = X['MedInc'] / (X['AveOccup'] + 1e-5)\n",
    "\n",
    "# Rimuovi inf/nan generati da rapporti\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. SELEZIONE FEATURE (Lasso con RobustScaler)\n",
    "# ---------------------------------------------------------\n",
    "print(\"3. Selezione Feature...\")\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "lasso_selector = SelectFromModel(LassoCV(cv=3, random_state=42, n_jobs=-1, max_iter=3000))\n",
    "lasso_selector.fit(X_scaled, y)\n",
    "support = lasso_selector.get_support()\n",
    "X_selected = X.loc[:, support]\n",
    "print(f\"   -> Feature Mantenute: {X_selected.shape[1]} (su {X.shape[1]})\")\n",
    "\n",
    "# Split Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. OPTUNA + CV PER SINGOLO MODELLO\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n4. Tuning XGBoost con 3-Fold CV (veloce e robusto)...\")\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "N_TRIALS = 40  # aumenta a 80-120 se vuoi spremere di più\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 600, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.08),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.95),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 3.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.5, 5.0),\n",
    "        'n_jobs': -1, 'random_state': 42\n",
    "    }\n",
    "    model = XGBRegressor(**params)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kf,\n",
    "                             scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=N_TRIALS)\n",
    "print(f\"   -> Best XGB CV RMSE: {study_xgb.best_value:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. STACKING (snello) CON BEST PARAMS\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n5. Stacking Ensemble Finale...\")\n",
    "\n",
    "best_xgb = study_xgb.best_params\n",
    "best_xgb.update({'n_jobs': -1, 'random_state': 42})\n",
    "\n",
    "# Parametri ragionevoli per LGBM e Cat (non ottimizzati per velocità)\n",
    "best_lgbm = {\n",
    "    'n_estimators': 900, 'learning_rate': 0.05, 'num_leaves': 60,\n",
    "    'feature_fraction': 0.85, 'bagging_fraction': 0.85, 'bagging_freq': 5,\n",
    "    'reg_alpha': 0.5, 'reg_lambda': 1.5, 'n_jobs': -1, 'random_state': 42\n",
    "}\n",
    "best_cat = {\n",
    "    'iterations': 900, 'learning_rate': 0.05, 'depth': 8,\n",
    "    'l2_leaf_reg': 4, 'subsample': 0.85, 'verbose': 0, 'random_state': 42\n",
    "}\n",
    "\n",
    "estimators = [\n",
    "    ('xgb', XGBRegressor(**best_xgb)),\n",
    "    ('lgbm', LGBMRegressor(**best_lgbm)),\n",
    "    ('cat', CatBoostRegressor(**best_cat))\n",
    "]\n",
    "\n",
    "stacking = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=RidgeCV(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. VALUTAZIONE\n",
    "# ---------------------------------------------------------\n",
    "y_pred = stacking.predict(X_test)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "final_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n==========================================\")\n",
    "print(\" FINAL TEST SET RESULTS (STACKING)\")\n",
    "print(\"==========================================\")\n",
    "print(f\" RMSE: {final_rmse:.5f}\")\n",
    "print(f\" R^2 : {final_r2:.5f}\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "# Facoltativo: pesi del meta-learner\n",
    "meta_model = stacking.final_estimator_\n",
    "model_names = [name for name, _ in stacking.estimators]\n",
    "print(\"\\nPesi meta-learner (RidgeCV):\")\n",
    "print(pd.DataFrame({'Model': model_names, 'Weight': meta_model.coef_}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
